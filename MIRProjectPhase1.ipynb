{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTWsrYobJnd3"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\"><font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=30>\n",
    "<p></p><p></p>\n",
    "به نام خدا\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<img src=\"Images/sharif.png\" width=\"25%\">\n",
    "<font color=blue>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر سلیمانی\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=green>\n",
    "فاز اول پروژه - سیستم بازیابی اطلاعات داده‌های ویکی‌پدیای فارسی\n",
    "</font>\n",
    "<p></p>\n",
    "<font color=#FF7500>\n",
    "بهار ۹۹\n",
    "<br>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXCFFyc8Jnd7"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>مقدمه </div>\n",
    "</font>\n",
    "<hr>\n",
    "در فاز اول پروژه درس بازیابی پیشرفته اطلاعات، شما باید سیستم بازیابی اطلاعات را برای مجموعه داده‌های ویکی پدیای فارسی پیاده سازی کنید. بدین صورت که مجموعه داده‌هایی که در اختیارتان قرار داده شده را پس از پردازش اولیه و نمایه‌سازی، آماده جستجو عبارات در آن کنید. سعی شده‌است که امکانات خواسته شده در این سیستم متناسب با جست‌وجو‌های کاربردی بر روی داده‌ها باشد.\n",
    "<br>\n",
    "پروژه از ۴ بخش تشکیل شده،‌ بخش اول آن آماده‌سازی اولیه داده‌هاست. پیشنهاد می شود برای پیاده‌سازی این بخش از کتابخانه هضم که توضیحات استفاده از آن در \n",
    "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
    "آمده است، استفاده کنید. بخش دوم، طراحی و پیاده‌سازی نمایه‌ساز برای داده‌هاست که با گرفتن داده‌های ورودی، نمایه‌ها و داده‌ساختار‌های مورد نیاز برای جستجو اسناد و دیگر نیازمندی‌های سیستم را تولید می‌کند. در بخش سوم می‌بایست امکان جستجو بر روی داده‌ساختار خروجی بخش قبلی را براساس مدل فضای برداری فراهم کنید. در این قسمت عبارت مورد جستجو در صورت دارا بودن غلط املایی باید اصلاح شود. در بخش آخر نیز با استفاده از پرسمان‌ها و اسنادی که به عنوان اسناد مرتبط به آن پرسمان معرفی شده، می‌بایست سیستم بازیابی خود را با استفاده از ۴ معیار ذکر شده در این بخش\n",
    "ارزیابی کنید.\n",
    "<br>\n",
    "در این دفترچه جوپیتر برای هر یک از چهار بخش پروژه، قسمت مجزایی در نظر گرفته شده‌است. شما باید کدهای خود را طوری بزنید که این بخش‌ها طبق توضیح به تفضیل آمده در هر بخش، به درستی کار کنند. کد‌های خود را می‌توانید در بخش‌های اضافه شده توسط خودتان در همین دفترچه جوپیتر بنویسید یا فایل‌های پایتون مربوط به پیاده‌سازی خود را در کنار دفترچه گذاشته و در بخش‌های مختلف این دفترچه بااستفاده از \n",
    "import\n",
    "مناسب از کد‌هایتان استفاده کنید.\n",
    "<br>\n",
    "در نهایت توجه کنید که دو بخش از این فاز پروژه به عنوان قسمت امتیازی برای شما در نظر گرفته شده. در این سند، بخش‌های امتیازی با علامت (*امتیازی*) مشخص شده‌اند. هر کدام از این بخش ها 10 نمره دارند.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-nJxAxdJnd8"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>مجموعه دادگان</div>\n",
    "</font>\n",
    "<hr>\n",
    "مجموعه دادگان مورد استفاده در این پروژه از جمع آوری اطلاعات موجود در صفحات ویکی پدیای فارسی به وجود آمده است.\n",
    "این مجموعه اسناد از دو بخش تشکیل شده است\n",
    ".\n",
    "<br>\n",
    "بخش اول که در فایل \n",
    "Persian.xml\n",
    "آمده است، شامل ۱۵۰۰ سند می‌باشد.\n",
    "هر سند شامل شناسه\n",
    "(id)،\n",
    "عنوان\n",
    "(title)،   \n",
    "و متن \n",
    "(text)\n",
    "است.\n",
    "بخش دوم که در پوشه‌ی \n",
    "queries\n",
    "آمده‌است، شامل تعدادی پرسمان است که برای سنجش سیستم‌ پیاده سازی شده‌ی شما مورد استفاده قرار خواهد گرفت.\n",
    "بخش سوم که در پوشه‌ی\n",
    "relevance\n",
    "آمده‌است،\n",
    "شامل یک فایل است که شناسه سند‌های مرتبط با هر پرسمان در آن آمده‌است.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avT4ky8EJnd-"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\" ><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>(10 نمره) بخش اول: آماده‌سازی اولیه داده‌ها</div>\n",
    "</font>\n",
    "<hr>\n",
    "هدف از این بخش اعمال عملیات متنی اولیه بر روی متن خام ورودی است تا کلمات به شکل مناسب برای قرارگیری در نمایه استخراج شوند. برای تسهیل این بخش شما می‌توانید از توابع کتابخانه‌ی هضم که توضیح استفاده از آن در \n",
    "<a href=\"http://www.sobhe.ir/hazm/\">این صفحه</a>\n",
    "آمده است استفاده‌ نمایید. همین طور در صورت نیاز به توضیحات بیشتر در خصوص این کتاب‌خانه می‌توانید به توضیحات مربوط به پروژه‌ی سه سال قبل از طریق\n",
    "<a href=\"http://ce.sharif.edu/courses/95-96/1/ce324-1/assignments/files/assignDir/MIR_Project1.pdf\">این صفحه</a>\n",
    "مراجعه کنید.\n",
    "<br>\n",
    "<br>\n",
    "عملیات مورد انتظار:\n",
    "<ol>\n",
    "    <li>\n",
    "یکسان‌سازی متن: یکی از عملیات مهم در پردازش متون به خصوص در زبان فارسی این عملیات\n",
    "است که شامل یکسان‌سازی استفاده از فاصله و نیم‌فاصله و نحوه‌ی شکستن یا ادغام کلمات و ... است.  به طور مثال، یک مورد از این یکسان‌سازی‌ها نحوه‌ی قرار گیری حرف جمع «ها» در انتهای کلمات جمع است که می‌تواند بدون فاصله چسبیده به کلمه، با یک فاصله‌ی کامل و یا با نیم‌فاصله\n",
    "پس از کلمه بیاید (کتابها، کتاب ها، کتاب‌ها)\n",
    "    </li>\n",
    "    <li> \n",
    "جدا کردن کلمات یک جمله: واحد متن مورد استفاده‌ی ما در ساخت نمایه و همین طور جست‌وجو در یک سیستم اطلاعاتی کلمات هستند. بنابر این جملات ورودی را باید بتوانیم به کلمات آن بشکنیم  و عملیات مورد نیاز را بر روی کلمات انجام دهیم.\n",
    "    </li>\n",
    "    <li>\n",
    "حذف علائم نگارشی: علائم نگارشی مانند نقطه، ویرگول، و ... باید از درون اسناد حذف شوند تا درون نمایه و جست‌وجو‌ها تاثیر نگذارند.\n",
    "    </li>\n",
    "<li>\n",
    "بازگرداندن کلمات به ریشه: عملیات دیگری که روی کلمات متن صورت میگیرد عمل بازگردانی به\n",
    "ریشه\n",
    "(stemming)\n",
    "است تا کلماتی که از یک ریشه هستند همگی یک کلمه به حساب بیاید.\n",
    "    </li>\n",
    "</ol>\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzEhdYtzJnd_"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در این بخش که برای آماده‌سازی اولیه متن داده‌هاست، تابع \n",
    "prepare_text\n",
    "باید طوری بر روی متن ورودی با نام\n",
    "raw_text\n",
    "عمل کند که\n",
    "عملیات‌های مورد انتظار ذکر شده روی متن انجام شود و متن آماده‌شده به عنوان خروجی تابع برگردانده شود. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import hazm as hz\n",
    "import os.path\n",
    "import numpy as np\n",
    "import string\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import glob\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZ98SbQMJneB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text:\n",
      "سلام به همگی\n",
      "['سلا', 'به', 'همگ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepare_text(raw_text):\n",
    "    n = hz.Normalizer()\n",
    "    s = hz.Stemmer()\n",
    "    normalized_text = n.normalize(raw_text)\n",
    "    normalized_text = n.punctuation_spacing(normalized_text)\n",
    "    tokenized_text = hz.word_tokenize(normalized_text)\n",
    "    for i in range(len(tokenized_text)):\n",
    "        tokenized_text[i] = tokenized_text[i].translate(str.maketrans(\"\", \"\", string.punctuation + \"٬«»؛،؟\"))\n",
    "        tokenized_text[i] = s.stem(tokenized_text[i])\n",
    "\n",
    "    prepared_text = []\n",
    "    for i in range(len(tokenized_text)):\n",
    "        if len(tokenized_text[i]) != 0:\n",
    "            prepared_text.append(tokenized_text[i])\n",
    "    return prepared_text\n",
    "\n",
    "print('Enter text:')\n",
    "raw_text = input()\n",
    "print(prepare_text(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "mlwmeLaQJneG"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (30 نمره) بخش دوم: ساخت نمایه</div>\n",
    "</font>\n",
    "<hr>\n",
    "در این بخش شما باید نمایه‌گذاری‌های مورد نیاز برای بخش جست‌وجو را انجام دهید. تمامی نمایه‌ها باید به صورت پویا باشند به این معنی که با حذف و یا اضافه کردن سندی در طول اجرای برنامه، سند از نمایه حذف شده و یا به آن اضافه شود. \n",
    "<br>\n",
    "شرح نمایه‌های مورد انتظار:\n",
    "<br>\n",
    "<ol>\n",
    "<li>\n",
    "با توجه به مواردی که در بخش بعد می‌آید و نیاز به جست‌وجو‌ی مجزا و با امتیازدهی متفاوت بر روی بخش‌های مختلف سند مثل عنوان یا متن آن، در این قسمت بایستی نمایه‌ی مناسب برای امکان جست‌وجو‌ در بخش‌های مختلف را پیاده‌سازی کنید. با استفاده از نمایه‌ی ساخته‌شده باید بتوان شماره تمامی اسنادی که یک کلمه در آن آمده است و همچنین همه جایگاه‌های این کلمه در هر بخش هر سند را پیدا کرد. انتخاب داده‌ساختار مناسب برای ذخیره نمایه بر عهده خودتان است\n",
    "(البته روش استفاده شده باید مبتنی بر موارد معرفی شده در کلاس باشد.).\n",
    "همچنین باید قادر باشید نمایه‌ها را در فایلی ذخیره کرده و از فایل ذخیره شده بازیابی کنید\n",
    "</li>\n",
    "<li>\n",
    "(*امتیازی*)\n",
    "نمایه‌ی \n",
    "Bigram: \n",
    "با استفاده از این نمایه می‌توان با دادن یک \n",
    "Bigram\n",
    "(ترکیب‌های دو حرفی) \n",
    "تمامی کلمات موجود در لغتنامه که این ترکیب در آنها موجود است را دریافت کرد. این نمایه برای قسمت اصلاح پرسمان که در بخش بعد توضیح داده خواهد شد، مورد استفاده قرار خواهد گرفت. توجه کنید که با حذف یک سند، تمامی کلمات موجود در آن از لغتنامه حذف نمی‌شوند زیرا ممکن است که آن کلمه در سند دیگری نیز آمده باشد. حذف یک کلمه را در صورتی انجام دهید که لیست آن در نمایه‌ی قسمت قبل خالی شده باشد.\n",
    "</li>\n",
    "</ol>\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjpoyRv9JneH"
   },
   "source": [
    " <div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش مربوط به ساخت نمایه‌هاست. تابع \n",
    "construct_indexes\n",
    "با گرفتن مسیر مجموعه‌داده‌ها\n",
    "اقدام به ساختن دو نمایه‌ی شرح داده‌شده می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_akK-pvJneI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3014\n",
      "1 3016\n",
      "2 3017\n",
      "3 3021\n",
      "4 3022\n",
      "5 3023\n",
      "6 3026\n",
      "7 3027\n",
      "8 3029\n",
      "9 3030\n",
      "10 3033\n",
      "11 3036\n",
      "12 3037\n",
      "13 3039\n",
      "14 3041\n",
      "15 3043\n",
      "16 3046\n",
      "17 3047\n",
      "18 3049\n",
      "19 3055\n",
      "20 3056\n",
      "21 3058\n",
      "22 3059\n",
      "23 3060\n",
      "24 3061\n",
      "25 3065\n",
      "26 3068\n",
      "27 3069\n",
      "28 3070\n",
      "29 3071\n",
      "30 3072\n",
      "31 3073\n",
      "32 3074\n",
      "33 3076\n",
      "34 3091\n",
      "35 3095\n",
      "36 3098\n",
      "37 3099\n",
      "38 3100\n",
      "39 3101\n",
      "40 3103\n",
      "41 3111\n",
      "42 3117\n",
      "43 3118\n",
      "44 3119\n",
      "45 3120\n",
      "46 3121\n",
      "47 3128\n",
      "48 3129\n",
      "49 3130\n",
      "50 3141\n",
      "51 3157\n",
      "52 3179\n",
      "53 3197\n",
      "54 3199\n",
      "55 3205\n",
      "56 3217\n",
      "57 3219\n",
      "58 3220\n",
      "59 3227\n",
      "60 3229\n",
      "61 3233\n",
      "62 3243\n",
      "63 3248\n",
      "64 3251\n",
      "65 3252\n",
      "66 3260\n",
      "67 3261\n",
      "68 3263\n",
      "69 3273\n",
      "70 3274\n",
      "71 3276\n",
      "72 3277\n",
      "73 3280\n",
      "74 3282\n",
      "75 3283\n",
      "76 3284\n",
      "77 3285\n",
      "78 3286\n",
      "79 3287\n",
      "80 3288\n",
      "81 3289\n",
      "82 3290\n",
      "83 3291\n",
      "84 3292\n",
      "85 3293\n",
      "86 3294\n",
      "87 3295\n",
      "88 3296\n",
      "89 3297\n",
      "90 3298\n",
      "91 3300\n",
      "92 3301\n",
      "93 3302\n",
      "94 3303\n",
      "95 3304\n",
      "96 3305\n",
      "97 3307\n",
      "98 3308\n",
      "99 3309\n",
      "100 3310\n",
      "101 3311\n",
      "102 3312\n",
      "103 3313\n",
      "104 3314\n",
      "105 3315\n",
      "106 3316\n",
      "107 3317\n",
      "108 3318\n",
      "109 3319\n",
      "110 3320\n",
      "111 3324\n",
      "112 3325\n",
      "113 3329\n",
      "114 3331\n",
      "115 3332\n",
      "116 3333\n",
      "117 3334\n",
      "118 3335\n",
      "119 3339\n",
      "120 3341\n",
      "121 3342\n",
      "122 3343\n",
      "123 3345\n",
      "124 3347\n",
      "125 3348\n",
      "126 3359\n",
      "127 3360\n",
      "128 3361\n",
      "129 3362\n",
      "130 3367\n",
      "131 3373\n",
      "132 3376\n",
      "133 3377\n",
      "134 3380\n",
      "135 3381\n",
      "136 3385\n",
      "137 3394\n",
      "138 3397\n",
      "139 3400\n",
      "140 3402\n",
      "141 3404\n",
      "142 3406\n",
      "143 3409\n",
      "144 3410\n",
      "145 3411\n",
      "146 3413\n",
      "147 3414\n",
      "148 3415\n",
      "149 3416\n",
      "150 3429\n",
      "151 3433\n",
      "152 3443\n",
      "153 3456\n",
      "154 3458\n",
      "155 3463\n",
      "156 3507\n",
      "157 3521\n",
      "158 3539\n",
      "159 3576\n",
      "160 3580\n",
      "161 3582\n",
      "162 3653\n",
      "163 3654\n",
      "164 3656\n",
      "165 3658\n",
      "166 3664\n",
      "167 3665\n",
      "168 3666\n",
      "169 3667\n",
      "170 3670\n",
      "171 3671\n",
      "172 3672\n",
      "173 3680\n",
      "174 3681\n",
      "175 3690\n",
      "176 3695\n",
      "177 3699\n",
      "178 3702\n",
      "179 3711\n",
      "180 3716\n",
      "181 3719\n",
      "182 3721\n",
      "183 3727\n",
      "184 3730\n",
      "185 3733\n",
      "186 3738\n",
      "187 3739\n",
      "188 3740\n",
      "189 3741\n",
      "190 3742\n",
      "191 3743\n",
      "192 3744\n",
      "193 3747\n",
      "194 3750\n",
      "195 3751\n",
      "196 3752\n",
      "197 3758\n",
      "198 3760\n",
      "199 3761\n",
      "200 3776\n",
      "201 3777\n",
      "202 3778\n",
      "203 3780\n",
      "204 3782\n",
      "205 3784\n",
      "206 3785\n",
      "207 3787\n",
      "208 3790\n",
      "209 3797\n",
      "210 3798\n",
      "211 3800\n",
      "212 3801\n",
      "213 3802\n",
      "214 3803\n",
      "215 3804\n",
      "216 3809\n",
      "217 3814\n",
      "218 3820\n",
      "219 3821\n",
      "220 3826\n",
      "221 3829\n",
      "222 3835\n",
      "223 3836\n",
      "224 3838\n",
      "225 3841\n",
      "226 3842\n",
      "227 3843\n",
      "228 3844\n",
      "229 3845\n",
      "230 3847\n",
      "231 3849\n",
      "232 3851\n",
      "233 3852\n",
      "234 3853\n",
      "235 3854\n",
      "236 3855\n",
      "237 3860\n",
      "238 3862\n",
      "239 3865\n",
      "240 3866\n",
      "241 3868\n",
      "242 3871\n",
      "243 3872\n",
      "244 3874\n",
      "245 3879\n",
      "246 3880\n",
      "247 3881\n",
      "248 3884\n",
      "249 3885\n",
      "250 3886\n",
      "251 3889\n",
      "252 3894\n",
      "253 3897\n",
      "254 3901\n",
      "255 3903\n",
      "256 3904\n",
      "257 3905\n",
      "258 3906\n",
      "259 3907\n",
      "260 3908\n",
      "261 3910\n",
      "262 3911\n",
      "263 3912\n",
      "264 3915\n",
      "265 3916\n",
      "266 3918\n",
      "267 3919\n",
      "268 3920\n",
      "269 3921\n",
      "270 3923\n",
      "271 3925\n",
      "272 3926\n",
      "273 3929\n",
      "274 3930\n",
      "275 3931\n",
      "276 3932\n",
      "277 3933\n",
      "278 3934\n",
      "279 3936\n",
      "280 3938\n",
      "281 3939\n",
      "282 3941\n",
      "283 3944\n",
      "284 3945\n",
      "285 3946\n",
      "286 3951\n",
      "287 3954\n",
      "288 3955\n",
      "289 3956\n",
      "290 3961\n",
      "291 3965\n",
      "292 3967\n",
      "293 3968\n",
      "294 3971\n",
      "295 3975\n",
      "296 3978\n",
      "297 3979\n",
      "298 3980\n",
      "299 3983\n",
      "300 3986\n",
      "301 3987\n",
      "302 3989\n",
      "303 3990\n",
      "304 3992\n",
      "305 3993\n",
      "306 3995\n",
      "307 3996\n",
      "308 3997\n",
      "309 4002\n",
      "310 4007\n",
      "311 4008\n",
      "312 4009\n",
      "313 4017\n",
      "314 4018\n",
      "315 4021\n",
      "316 4023\n",
      "317 4024\n",
      "318 4025\n",
      "319 4026\n",
      "320 4027\n",
      "321 4028\n",
      "322 4030\n",
      "323 4032\n",
      "324 4034\n",
      "325 4036\n",
      "326 4037\n",
      "327 4039\n",
      "328 4040\n",
      "329 4041\n",
      "330 4042\n",
      "331 4048\n",
      "332 4049\n",
      "333 4050\n",
      "334 4051\n",
      "335 4054\n",
      "336 4055\n",
      "337 4056\n",
      "338 4057\n",
      "339 4058\n",
      "340 4059\n",
      "341 4062\n",
      "342 4065\n",
      "343 4066\n",
      "344 4069\n",
      "345 4070\n",
      "346 4072\n",
      "347 4074\n",
      "348 4080\n",
      "349 4081\n",
      "350 4082\n",
      "351 4086\n",
      "352 4088\n",
      "353 4094\n",
      "354 4100\n",
      "355 4102\n",
      "356 4103\n",
      "357 4104\n",
      "358 4105\n",
      "359 4106\n",
      "360 4107\n",
      "361 4109\n",
      "362 4110\n",
      "363 4111\n",
      "364 4112\n",
      "365 4113\n",
      "366 4114\n",
      "367 4115\n",
      "368 4118\n",
      "369 4120\n",
      "370 4131\n",
      "371 4133\n",
      "372 4135\n",
      "373 4139\n",
      "374 4140\n",
      "375 4141\n",
      "376 4142\n",
      "377 4143\n",
      "378 4144\n",
      "379 4145\n",
      "380 4146\n",
      "381 4147\n",
      "382 4148\n",
      "383 4149\n",
      "384 4151\n",
      "385 4152\n",
      "386 4153\n",
      "387 4154\n",
      "388 4155\n",
      "389 4156\n",
      "390 4157\n",
      "391 4158\n",
      "392 4159\n",
      "393 4160\n",
      "394 4161\n",
      "395 4162\n",
      "396 4163\n",
      "397 4164\n",
      "398 4165\n",
      "399 4166\n",
      "400 4167\n",
      "401 4168\n",
      "402 4169\n",
      "403 4172\n",
      "404 4183\n",
      "405 4190\n",
      "406 4191\n",
      "407 4204\n",
      "408 4205\n",
      "409 4206\n",
      "410 4207\n",
      "411 4208\n",
      "412 4209\n",
      "413 4210\n",
      "414 4211\n",
      "415 4212\n",
      "416 4213\n",
      "417 4214\n",
      "418 4215\n",
      "419 4216\n",
      "420 4217\n",
      "421 4218\n",
      "422 4219\n",
      "423 4220\n",
      "424 4221\n",
      "425 4222\n",
      "426 4223\n",
      "427 4224\n",
      "428 4225\n",
      "429 4226\n",
      "430 4227\n",
      "431 4228\n",
      "432 4229\n",
      "433 4230\n",
      "434 4231\n",
      "435 4240\n",
      "436 4247\n",
      "437 4248\n",
      "438 4254\n",
      "439 4255\n",
      "440 4256\n",
      "441 4257\n",
      "442 4259\n",
      "443 4263\n",
      "444 4269\n",
      "445 4274\n",
      "446 4275\n",
      "447 4281\n",
      "448 4282\n",
      "449 4283\n",
      "450 4284\n",
      "451 4285\n",
      "452 4286\n",
      "453 4287\n",
      "454 4288\n",
      "455 4289\n",
      "456 4290\n",
      "457 4291\n",
      "458 4292\n",
      "459 4293\n",
      "460 4294\n",
      "461 4295\n",
      "462 4296\n",
      "463 4297\n",
      "464 4299\n",
      "465 4300\n",
      "466 4301\n",
      "467 4306\n",
      "468 4313\n",
      "469 4321\n",
      "470 4323\n",
      "471 4329\n",
      "472 4332\n",
      "473 4335\n",
      "474 4337\n",
      "475 4339\n",
      "476 4363\n",
      "477 4368\n",
      "478 4375\n",
      "479 4376\n",
      "480 4377\n",
      "481 4379\n",
      "482 4382\n",
      "483 4383\n",
      "484 4385\n",
      "485 4388\n",
      "486 4391\n",
      "487 4394\n",
      "488 4395\n",
      "489 4396\n",
      "490 4397\n",
      "491 4398\n",
      "492 4399\n",
      "493 4400\n",
      "494 4401\n",
      "495 4402\n",
      "496 4404\n",
      "497 4406\n",
      "498 4408\n",
      "499 4409\n",
      "500 4410\n",
      "501 4415\n",
      "502 4416\n",
      "503 4417\n",
      "504 4422\n",
      "505 4423\n",
      "506 4425\n",
      "507 4427\n",
      "508 4438\n",
      "509 4439\n",
      "510 4443\n",
      "511 4444\n",
      "512 4446\n",
      "513 4447\n",
      "514 4451\n",
      "515 4455\n",
      "516 4459\n",
      "517 4460\n",
      "518 4462\n",
      "519 4463\n",
      "520 4466\n",
      "521 4468\n",
      "522 4470\n",
      "523 4471\n",
      "524 4473\n",
      "525 4475\n",
      "526 4476\n",
      "527 4484\n",
      "528 4485\n",
      "529 4486\n",
      "530 4487\n",
      "531 4488\n",
      "532 4489\n",
      "533 4492\n",
      "534 4493\n",
      "535 4497\n",
      "536 4503\n",
      "537 4505\n",
      "538 4506\n",
      "539 4508\n",
      "540 4509\n",
      "541 4512\n",
      "542 4513\n",
      "543 4514\n",
      "544 4515\n",
      "545 4516\n",
      "546 4517\n",
      "547 4518\n",
      "548 4519\n",
      "549 4524\n",
      "550 4525\n",
      "551 4526\n",
      "552 4527\n",
      "553 4528\n",
      "554 4530\n",
      "555 4531\n",
      "556 4532\n",
      "557 4533\n",
      "558 4534\n",
      "559 4536\n",
      "560 4537\n",
      "561 4539\n",
      "562 4540\n",
      "563 4541\n",
      "564 4542\n",
      "565 4543\n",
      "566 4544\n",
      "567 4545\n",
      "568 4546\n",
      "569 4547\n",
      "570 4548\n",
      "571 4549\n",
      "572 4550\n",
      "573 4551\n",
      "574 4554\n",
      "575 4555\n",
      "576 4556\n",
      "577 4557\n",
      "578 4560\n",
      "579 4561\n",
      "580 4563\n",
      "581 4564\n",
      "582 4568\n",
      "583 4570\n",
      "584 4571\n",
      "585 4573\n",
      "586 4576\n",
      "587 4577\n",
      "588 4582\n",
      "589 4586\n",
      "590 4589\n",
      "591 4593\n",
      "592 4595\n",
      "593 4597\n",
      "594 4598\n",
      "595 4601\n",
      "596 4603\n",
      "597 4605\n",
      "598 4609\n",
      "599 4611\n",
      "600 4612\n",
      "601 4621\n",
      "602 4624\n",
      "603 4625\n",
      "604 4627\n",
      "605 4628\n",
      "606 4629\n",
      "607 4636\n",
      "608 4637\n",
      "609 4638\n",
      "610 4646\n",
      "611 4650\n",
      "612 4655\n",
      "613 4656\n",
      "614 4658\n",
      "615 4659\n",
      "616 4662\n",
      "617 4665\n",
      "618 4666\n",
      "619 4668\n",
      "620 4669\n",
      "621 4670\n",
      "622 4671\n",
      "623 4676\n",
      "624 4677\n",
      "625 4678\n",
      "626 4679\n",
      "627 4680\n",
      "628 4681\n",
      "629 4683\n",
      "630 4684\n",
      "631 4686\n",
      "632 4687\n",
      "633 4690\n",
      "634 4691\n",
      "635 4707\n",
      "636 4710\n",
      "637 4718\n",
      "638 4719\n",
      "639 4720\n",
      "640 4725\n",
      "641 4726\n",
      "642 4730\n",
      "643 4731\n",
      "644 4732\n",
      "645 4734\n",
      "646 4736\n",
      "647 4737\n",
      "648 4742\n",
      "649 4743\n",
      "650 4745\n",
      "651 4746\n",
      "652 4748\n",
      "653 4752\n",
      "654 4755\n",
      "655 4757\n",
      "656 4758\n",
      "657 4760\n",
      "658 4761\n",
      "659 4762\n",
      "660 4763\n",
      "661 4765\n",
      "662 4766\n",
      "663 4777\n",
      "664 4778\n",
      "665 4779\n",
      "666 4780\n",
      "667 4781\n",
      "668 4784\n",
      "669 4799\n",
      "670 4805\n",
      "671 4811\n",
      "672 4815\n",
      "673 4816\n",
      "674 4818\n",
      "675 4819\n",
      "676 4820\n",
      "677 4824\n",
      "678 4831\n",
      "679 4833\n",
      "680 4838\n",
      "681 4839\n",
      "682 4840\n",
      "683 4843\n",
      "684 4846\n",
      "685 4847\n",
      "686 4849\n",
      "687 4856\n",
      "688 4858\n",
      "689 4864\n",
      "690 4865\n",
      "691 4867\n",
      "692 4872\n",
      "693 4874\n",
      "694 4876\n",
      "695 4878\n",
      "696 4893\n",
      "697 4896\n",
      "698 4897\n",
      "699 4898\n",
      "700 4899\n",
      "701 4901\n",
      "702 4903\n",
      "703 4905\n",
      "704 4906\n",
      "705 4907\n",
      "706 4908\n",
      "707 4909\n",
      "708 4910\n",
      "709 4911\n",
      "710 4912\n",
      "711 4914\n",
      "712 4916\n",
      "713 4918\n",
      "714 4919\n",
      "715 4929\n",
      "716 4938\n",
      "717 4940\n",
      "718 4941\n",
      "719 4942\n",
      "720 4953\n",
      "721 4955\n",
      "722 4986\n",
      "723 4990\n",
      "724 4992\n",
      "725 4993\n",
      "726 4996\n",
      "727 4997\n",
      "728 4998\n",
      "729 5010\n",
      "730 5011\n",
      "731 5012\n",
      "732 5028\n",
      "733 5031\n",
      "734 5039\n",
      "735 5048\n",
      "736 5049\n",
      "737 5050\n",
      "738 5052\n",
      "739 5053\n",
      "740 5067\n",
      "741 5071\n",
      "742 5122\n",
      "743 5126\n",
      "744 5134\n",
      "745 5141\n",
      "746 5147\n",
      "747 5152\n",
      "748 5153\n",
      "749 5173\n",
      "750 5174\n",
      "751 5185\n",
      "752 5187\n",
      "753 5189\n",
      "754 5192\n",
      "755 5195\n",
      "756 5201\n",
      "757 5203\n",
      "758 5206\n",
      "759 5236\n",
      "760 5237\n",
      "761 5238\n",
      "762 5240\n",
      "763 5241\n",
      "764 5252\n",
      "765 5255\n",
      "766 5256\n",
      "767 5257\n",
      "768 5260\n",
      "769 5261\n",
      "770 5262\n",
      "771 5263\n",
      "772 5264\n",
      "773 5266\n",
      "774 5268\n",
      "775 5269\n",
      "776 5275\n",
      "777 5277\n",
      "778 5281\n",
      "779 5282\n",
      "780 5284\n",
      "781 5285\n",
      "782 5287\n",
      "783 5288\n",
      "784 5293\n",
      "785 5294\n",
      "786 5296\n",
      "787 5299\n",
      "788 5301\n",
      "789 5303\n",
      "790 5304\n",
      "791 5306\n",
      "792 5307\n",
      "793 5308\n",
      "794 5309\n",
      "795 5310\n",
      "796 5311\n",
      "797 5312\n",
      "798 5318\n",
      "799 5319\n",
      "800 5320\n",
      "801 5342\n",
      "802 5346\n",
      "803 5352\n",
      "804 5353\n",
      "805 5354\n",
      "806 5355\n",
      "807 5357\n",
      "808 5359\n",
      "809 5360\n",
      "810 5362\n",
      "811 5364\n",
      "812 5366\n",
      "813 5369\n",
      "814 5370\n",
      "815 5371\n",
      "816 5372\n",
      "817 5375\n",
      "818 5378\n",
      "819 5380\n",
      "820 5381\n",
      "821 5382\n",
      "822 5383\n",
      "823 5386\n",
      "824 5390\n",
      "825 5393\n",
      "826 5396\n",
      "827 5398\n",
      "828 5399\n",
      "829 5400\n",
      "830 5402\n",
      "831 5403\n",
      "832 5408\n",
      "833 5409\n",
      "834 5410\n",
      "835 5411\n",
      "836 5414\n",
      "837 5415\n",
      "838 5416\n",
      "839 5422\n",
      "840 5428\n",
      "841 5429\n",
      "842 5430\n",
      "843 5440\n",
      "844 5444\n",
      "845 5445\n",
      "846 5446\n",
      "847 5448\n",
      "848 5449\n",
      "849 5450\n",
      "850 5451\n",
      "851 5453\n",
      "852 5454\n",
      "853 5455\n",
      "854 5456\n",
      "855 5458\n",
      "856 5459\n",
      "857 5460\n",
      "858 5462\n",
      "859 5465\n",
      "860 5466\n",
      "861 5474\n",
      "862 5478\n",
      "863 5481\n",
      "864 5484\n",
      "865 5486\n",
      "866 5490\n",
      "867 5492\n",
      "868 5494\n",
      "869 5495\n",
      "870 5496\n",
      "871 5497\n",
      "872 5498\n",
      "873 5499\n",
      "874 5501\n",
      "875 5503\n",
      "876 5504\n",
      "877 5506\n",
      "878 5508\n",
      "879 5509\n",
      "880 5510\n",
      "881 5512\n",
      "882 5517\n",
      "883 5519\n",
      "884 5522\n",
      "885 5523\n",
      "886 5524\n",
      "887 5525\n",
      "888 5526\n",
      "889 5527\n",
      "890 5529\n",
      "891 5533\n",
      "892 5534\n",
      "893 5542\n",
      "894 5549\n",
      "895 5551\n",
      "896 5553\n",
      "897 5554\n",
      "898 5560\n",
      "899 5568\n",
      "900 5569\n",
      "901 5571\n",
      "902 5572\n",
      "903 5573\n",
      "904 5576\n",
      "905 5579\n",
      "906 5580\n",
      "907 5581\n",
      "908 5582\n",
      "909 5583\n",
      "910 5584\n",
      "911 5585\n",
      "912 5586\n",
      "913 5587\n",
      "914 5588\n",
      "915 5589\n",
      "916 5590\n",
      "917 5591\n",
      "918 5592\n",
      "919 5593\n",
      "920 5594\n",
      "921 5595\n",
      "922 5596\n",
      "923 5597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924 5598\n",
      "925 5600\n",
      "926 5601\n",
      "927 5602\n",
      "928 5603\n",
      "929 5604\n",
      "930 5605\n",
      "931 5607\n",
      "932 5608\n",
      "933 5610\n",
      "934 5611\n",
      "935 5612\n",
      "936 5613\n",
      "937 5615\n",
      "938 5616\n",
      "939 5619\n",
      "940 5620\n",
      "941 5636\n",
      "942 5644\n",
      "943 5660\n",
      "944 5665\n",
      "945 5668\n",
      "946 5669\n",
      "947 5678\n",
      "948 5679\n",
      "949 5680\n",
      "950 5681\n",
      "951 5683\n",
      "952 5686\n",
      "953 5690\n",
      "954 5693\n",
      "955 5694\n",
      "956 5696\n",
      "957 5698\n",
      "958 5700\n",
      "959 5701\n",
      "960 5702\n",
      "961 5703\n",
      "962 5704\n",
      "963 5705\n",
      "964 5707\n",
      "965 5708\n",
      "966 5709\n",
      "967 5710\n",
      "968 5714\n",
      "969 5720\n",
      "970 5721\n",
      "971 5723\n",
      "972 5724\n",
      "973 5725\n",
      "974 5727\n",
      "975 5728\n",
      "976 5729\n",
      "977 5731\n",
      "978 5735\n",
      "979 5739\n",
      "980 5740\n",
      "981 5746\n",
      "982 5748\n",
      "983 5749\n",
      "984 5759\n",
      "985 5763\n",
      "986 5765\n",
      "987 5766\n",
      "988 5767\n",
      "989 5772\n",
      "990 5773\n",
      "991 5776\n",
      "992 5777\n",
      "993 5779\n",
      "994 5781\n",
      "995 5784\n",
      "996 5786\n",
      "997 5787\n",
      "998 5793\n",
      "999 5805\n",
      "1000 5814\n",
      "1001 5815\n",
      "1002 5816\n",
      "1003 5817\n",
      "1004 5818\n",
      "1005 5820\n",
      "1006 5823\n",
      "1007 5824\n",
      "1008 5826\n",
      "1009 5830\n",
      "1010 5838\n",
      "1011 5841\n",
      "1012 5843\n",
      "1013 5844\n",
      "1014 5845\n",
      "1015 5846\n",
      "1016 5852\n",
      "1017 5858\n",
      "1018 5859\n",
      "1019 5861\n",
      "1020 5862\n",
      "1021 5864\n",
      "1022 5865\n",
      "1023 5866\n",
      "1024 5867\n",
      "1025 5868\n",
      "1026 5869\n",
      "1027 5870\n",
      "1028 5872\n",
      "1029 5878\n",
      "1030 5879\n",
      "1031 5883\n",
      "1032 5884\n",
      "1033 5885\n",
      "1034 5886\n",
      "1035 5887\n",
      "1036 5889\n",
      "1037 5902\n",
      "1038 5903\n",
      "1039 5904\n",
      "1040 5905\n",
      "1041 5907\n",
      "1042 5909\n",
      "1043 5910\n",
      "1044 5913\n",
      "1045 5916\n",
      "1046 5917\n",
      "1047 5928\n",
      "1048 5931\n",
      "1049 5932\n",
      "1050 5933\n",
      "1051 5937\n",
      "1052 5940\n",
      "1053 5943\n",
      "1054 5945\n",
      "1055 5948\n",
      "1056 5957\n",
      "1057 5966\n",
      "1058 5967\n",
      "1059 5977\n",
      "1060 5979\n",
      "1061 5980\n",
      "1062 5982\n",
      "1063 5984\n",
      "1064 5985\n",
      "1065 5987\n",
      "1066 5994\n",
      "1067 6011\n",
      "1068 6013\n",
      "1069 6014\n",
      "1070 6015\n",
      "1071 6016\n",
      "1072 6020\n",
      "1073 6021\n",
      "1074 6023\n",
      "1075 6024\n",
      "1076 6025\n",
      "1077 6030\n",
      "1078 6033\n",
      "1079 6034\n",
      "1080 6035\n",
      "1081 6036\n",
      "1082 6038\n",
      "1083 6039\n",
      "1084 6040\n",
      "1085 6042\n",
      "1086 6043\n",
      "1087 6044\n",
      "1088 6045\n",
      "1089 6046\n",
      "1090 6049\n",
      "1091 6052\n",
      "1092 6054\n",
      "1093 6055\n",
      "1094 6058\n",
      "1095 6061\n",
      "1096 6064\n",
      "1097 6070\n",
      "1098 6074\n",
      "1099 6075\n",
      "1100 6077\n",
      "1101 6078\n",
      "1102 6088\n",
      "1103 6089\n",
      "1104 6096\n",
      "1105 6100\n",
      "1106 6108\n",
      "1107 6109\n",
      "1108 6115\n",
      "1109 6119\n",
      "1110 6125\n",
      "1111 6127\n",
      "1112 6132\n",
      "1113 6133\n",
      "1114 6138\n",
      "1115 6140\n",
      "1116 6142\n",
      "1117 6152\n",
      "1118 6154\n",
      "1119 6155\n",
      "1120 6157\n",
      "1121 6159\n",
      "1122 6160\n",
      "1123 6161\n",
      "1124 6162\n",
      "1125 6163\n",
      "1126 6164\n",
      "1127 6166\n",
      "1128 6169\n",
      "1129 6172\n",
      "1130 6173\n",
      "1131 6174\n",
      "1132 6177\n",
      "1133 6178\n",
      "1134 6183\n",
      "1135 6185\n",
      "1136 6186\n",
      "1137 6187\n",
      "1138 6188\n",
      "1139 6190\n",
      "1140 6193\n",
      "1141 6196\n",
      "1142 6200\n",
      "1143 6202\n",
      "1144 6203\n",
      "1145 6207\n",
      "1146 6208\n",
      "1147 6209\n",
      "1148 6214\n",
      "1149 6215\n",
      "1150 6217\n",
      "1151 6218\n",
      "1152 6220\n",
      "1153 6223\n",
      "1154 6225\n",
      "1155 6226\n",
      "1156 6229\n",
      "1157 6230\n",
      "1158 6233\n",
      "1159 6235\n",
      "1160 6247\n",
      "1161 6249\n",
      "1162 6250\n",
      "1163 6251\n",
      "1164 6254\n",
      "1165 6256\n",
      "1166 6257\n",
      "1167 6258\n",
      "1168 6260\n",
      "1169 6264\n",
      "1170 6266\n",
      "1171 6272\n",
      "1172 6273\n",
      "1173 6275\n",
      "1174 6277\n",
      "1175 6283\n",
      "1176 6289\n",
      "1177 6294\n",
      "1178 6295\n",
      "1179 6296\n",
      "1180 6297\n",
      "1181 6301\n",
      "1182 6302\n",
      "1183 6303\n",
      "1184 6304\n",
      "1185 6307\n",
      "1186 6309\n",
      "1187 6312\n",
      "1188 6315\n",
      "1189 6324\n",
      "1190 6333\n",
      "1191 6339\n",
      "1192 6352\n",
      "1193 6354\n",
      "1194 6357\n",
      "1195 6359\n",
      "1196 6360\n",
      "1197 6361\n",
      "1198 6362\n",
      "1199 6363\n",
      "1200 6369\n",
      "1201 6372\n",
      "1202 6375\n",
      "1203 6380\n",
      "1204 6382\n",
      "1205 6394\n",
      "1206 6396\n",
      "1207 6402\n",
      "1208 6403\n",
      "1209 6404\n",
      "1210 6405\n",
      "1211 6415\n",
      "1212 6416\n",
      "1213 6417\n",
      "1214 6418\n",
      "1215 6419\n",
      "1216 6420\n",
      "1217 6423\n",
      "1218 6424\n",
      "1219 6426\n",
      "1220 6427\n",
      "1221 6428\n",
      "1222 6429\n",
      "1223 6434\n",
      "1224 6437\n",
      "1225 6446\n",
      "1226 6447\n",
      "1227 6452\n",
      "1228 6456\n",
      "1229 6457\n",
      "1230 6459\n",
      "1231 6461\n",
      "1232 6466\n",
      "1233 6471\n",
      "1234 6472\n",
      "1235 6475\n",
      "1236 6476\n",
      "1237 6477\n",
      "1238 6480\n",
      "1239 6482\n",
      "1240 6487\n",
      "1241 6489\n",
      "1242 6491\n",
      "1243 6492\n",
      "1244 6494\n",
      "1245 6495\n",
      "1246 6497\n",
      "1247 6502\n",
      "1248 6506\n",
      "1249 6507\n",
      "1250 6517\n",
      "1251 6521\n",
      "1252 6522\n",
      "1253 6527\n",
      "1254 6531\n",
      "1255 6535\n",
      "1256 6546\n",
      "1257 6548\n",
      "1258 6553\n",
      "1259 6558\n",
      "1260 6568\n",
      "1261 6572\n",
      "1262 6575\n",
      "1263 6580\n",
      "1264 6583\n",
      "1265 6586\n",
      "1266 6587\n",
      "1267 6589\n",
      "1268 6591\n",
      "1269 6595\n",
      "1270 6600\n",
      "1271 6602\n",
      "1272 6604\n",
      "1273 6606\n",
      "1274 6607\n",
      "1275 6609\n",
      "1276 6612\n",
      "1277 6613\n",
      "1278 6615\n",
      "1279 6616\n",
      "1280 6617\n",
      "1281 6618\n",
      "1282 6621\n",
      "1283 6623\n",
      "1284 6626\n",
      "1285 6627\n",
      "1286 6628\n",
      "1287 6629\n",
      "1288 6633\n",
      "1289 6634\n",
      "1290 6635\n",
      "1291 6636\n",
      "1292 6637\n",
      "1293 6638\n",
      "1294 6640\n",
      "1295 6641\n",
      "1296 6642\n",
      "1297 6645\n",
      "1298 6646\n",
      "1299 6647\n",
      "1300 6649\n",
      "1301 6650\n",
      "1302 6651\n",
      "1303 6652\n",
      "1304 6653\n",
      "1305 6654\n",
      "1306 6658\n",
      "1307 6662\n",
      "1308 6665\n",
      "1309 6667\n",
      "1310 6669\n",
      "1311 6672\n",
      "1312 6674\n",
      "1313 6677\n",
      "1314 6678\n",
      "1315 6682\n",
      "1316 6684\n",
      "1317 6685\n",
      "1318 6692\n",
      "1319 6694\n",
      "1320 6696\n",
      "1321 6699\n",
      "1322 6700\n",
      "1323 6701\n",
      "1324 6702\n",
      "1325 6703\n",
      "1326 6707\n",
      "1327 6708\n",
      "1328 6709\n",
      "1329 6710\n",
      "1330 6711\n",
      "1331 6716\n",
      "1332 6717\n",
      "1333 6718\n",
      "1334 6719\n",
      "1335 6720\n",
      "1336 6721\n",
      "1337 6722\n",
      "1338 6723\n",
      "1339 6724\n",
      "1340 6728\n",
      "1341 6731\n",
      "1342 6732\n",
      "1343 6734\n",
      "1344 6735\n",
      "1345 6737\n",
      "1346 6738\n",
      "1347 6739\n",
      "1348 6740\n",
      "1349 6741\n",
      "1350 6742\n",
      "1351 6744\n",
      "1352 6748\n",
      "1353 6749\n",
      "1354 6752\n",
      "1355 6753\n",
      "1356 6759\n",
      "1357 6760\n",
      "1358 6761\n",
      "1359 6762\n",
      "1360 6763\n",
      "1361 6764\n",
      "1362 6765\n",
      "1363 6766\n",
      "1364 6768\n",
      "1365 6769\n",
      "1366 6770\n",
      "1367 6771\n",
      "1368 6772\n",
      "1369 6774\n",
      "1370 6775\n",
      "1371 6776\n",
      "1372 6777\n",
      "1373 6778\n",
      "1374 6784\n",
      "1375 6788\n",
      "1376 6789\n",
      "1377 6791\n",
      "1378 6794\n",
      "1379 6798\n",
      "1380 6800\n",
      "1381 6802\n",
      "1382 6803\n",
      "1383 6804\n",
      "1384 6805\n",
      "1385 6806\n",
      "1386 6807\n",
      "1387 6809\n",
      "1388 6810\n",
      "1389 6812\n",
      "1390 6813\n",
      "1391 6814\n",
      "1392 6815\n",
      "1393 6817\n",
      "1394 6819\n",
      "1395 6821\n",
      "1396 6822\n",
      "1397 6823\n",
      "1398 6824\n",
      "1399 6825\n",
      "1400 6826\n",
      "1401 6827\n",
      "1402 6828\n",
      "1403 6830\n",
      "1404 6831\n",
      "1405 6835\n",
      "1406 6836\n",
      "1407 6837\n",
      "1408 6838\n",
      "1409 6840\n",
      "1410 6841\n",
      "1411 6844\n",
      "1412 6845\n",
      "1413 6846\n",
      "1414 6847\n",
      "1415 6848\n",
      "1416 6849\n",
      "1417 6850\n",
      "1418 6851\n",
      "1419 6854\n",
      "1420 6859\n",
      "1421 6860\n",
      "1422 6861\n",
      "1423 6862\n",
      "1424 6863\n",
      "1425 6864\n",
      "1426 6865\n",
      "1427 6867\n",
      "1428 6868\n",
      "1429 6870\n",
      "1430 6871\n",
      "1431 6872\n",
      "1432 6873\n",
      "1433 6874\n",
      "1434 6875\n",
      "1435 6876\n",
      "1436 6878\n",
      "1437 6879\n",
      "1438 6881\n",
      "1439 6883\n",
      "1440 6884\n",
      "1441 6885\n",
      "1442 6886\n",
      "1443 6887\n",
      "1444 6889\n",
      "1445 6890\n",
      "1446 6894\n",
      "1447 6896\n",
      "1448 6899\n",
      "1449 6900\n",
      "1450 6907\n",
      "1451 6909\n",
      "1452 6913\n",
      "1453 6915\n",
      "1454 6917\n",
      "1455 6919\n",
      "1456 6921\n",
      "1457 6922\n",
      "1458 6924\n",
      "1459 6930\n",
      "1460 6931\n",
      "1461 6932\n",
      "1462 6937\n",
      "1463 6938\n",
      "1464 6939\n",
      "1465 6940\n",
      "1466 6943\n",
      "1467 6944\n",
      "1468 6945\n",
      "1469 6949\n",
      "1470 6955\n",
      "1471 6957\n",
      "1472 6959\n",
      "1473 6962\n",
      "1474 6963\n",
      "1475 6964\n",
      "1476 6965\n",
      "1477 6966\n",
      "1478 6967\n",
      "1479 6969\n",
      "1480 6970\n",
      "1481 6973\n",
      "1482 6974\n",
      "1483 6976\n",
      "1484 6978\n",
      "1485 6979\n",
      "1486 6982\n",
      "1487 6983\n",
      "1488 6984\n",
      "1489 6985\n",
      "1490 6986\n",
      "1491 6987\n",
      "1492 6988\n",
      "1493 6991\n",
      "1494 6995\n",
      "1495 6997\n",
      "1496 6998\n",
      "1497 7002\n",
      "1498 7006\n",
      "1499 7008\n",
      "1500 7013\n",
      "1501 7016\n",
      "1502 7024\n",
      "1503 7025\n",
      "1504 7026\n",
      "1505 7029\n",
      "1506 7032\n",
      "1507 7033\n",
      "1508 7039\n",
      "1509 7040\n",
      "1510 7041\n",
      "1511 7048\n",
      "1512 7049\n",
      "1513 7050\n",
      "1514 7051\n",
      "1515 7052\n",
      "1516 7055\n",
      "1517 7056\n",
      "1518 7057\n",
      "1519 7058\n",
      "1520 7059\n",
      "1521 7060\n",
      "1522 7063\n",
      "1523 7065\n",
      "1524 7067\n",
      "1525 7068\n",
      "1526 7069\n",
      "1527 7070\n",
      "1528 7072\n",
      "1529 7077\n",
      "1530 7081\n",
      "1531 7084\n",
      "1532 7085\n",
      "1533 7094\n",
      "1534 7095\n",
      "1535 7098\n",
      "1536 7099\n",
      "1537 7100\n",
      "1538 7101\n",
      "1539 7102\n",
      "1540 7103\n",
      "1541 7104\n",
      "1542 7110\n",
      "1543 7113\n",
      "1544 7114\n",
      "1545 7122\n",
      "1546 7123\n",
      "1547 7125\n",
      "1548 7127\n",
      "1549 7128\n",
      "1550 7129\n",
      "1551 7130\n",
      "1552 7131\n",
      "1553 7132\n",
      "1554 7133\n",
      "1555 7134\n",
      "1556 7135\n",
      "1557 7136\n",
      "1558 7137\n",
      "1559 7138\n",
      "1560 7139\n",
      "1561 7141\n",
      "1562 7142\n",
      "1563 7143\n",
      "1564 7144\n",
      "1565 7145\n",
      "1566 7146\n",
      "1567 7147\n",
      "1568 7151\n",
      "1569 7153\n",
      "1570 7155\n",
      "1571 7156\n"
     ]
    }
   ],
   "source": [
    "def construct_positional_indexes(doc_path):\n",
    "    tree = ET.parse(doc_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    ids = root.findall(\n",
    "        \"./{http://www.mediawiki.org/xml/export-0.10/}page/{http://www.mediawiki.org/xml/export-0.10/}id\")\n",
    "    titles = root.findall(\n",
    "        \"./{http://www.mediawiki.org/xml/export-0.10/}page/{http://www.mediawiki.org/xml/export-0.10/}title\")\n",
    "    texts = root.findall(\n",
    "        \"./{http://www.mediawiki.org/xml/export-0.10/}page/{http://www.mediawiki.org/xml/export-0.10/}revision/{http://www.mediawiki.org/xml/export-0.10/}text\")\n",
    "    title_text_pair = {}\n",
    "    for i in range(len(ids)):\n",
    "        title_text_pair[ids[i].text] = {\"title\": titles[i].text, \"text\": texts[i].text}\n",
    "    tokenized_title_text_pair = {}\n",
    "\n",
    "    for i in range(len(ids)):\n",
    "        prepared_title = prepare_text(title_text_pair[ids[i].text][\"title\"])\n",
    "        prepared_text = prepare_text(title_text_pair[ids[i].text][\"text\"])\n",
    "        tokenized_title_text_pair[ids[i].text] = {\"title\": prepared_title, \"text\": prepared_text}\n",
    "        print(i, ids[i].text)\n",
    "\n",
    "    pos_index = {}\n",
    "    doc_ids = list(tokenized_title_text_pair.keys())\n",
    "    for i in range(len(tokenized_title_text_pair)):\n",
    "        doc_id = doc_ids[i]\n",
    "        title = tokenized_title_text_pair[doc_id][\"title\"]\n",
    "        text = tokenized_title_text_pair[doc_id][\"text\"]\n",
    "        doc_id_num = int(doc_id)\n",
    "\n",
    "        for pos, word in enumerate(title):\n",
    "            if word not in pos_index:\n",
    "                pos_index[word] = {doc_id : {\"title\": [pos]}}\n",
    "            else:\n",
    "                if doc_id in pos_index[word].keys():\n",
    "                    if \"title\" not in pos_index[word][doc_id].keys():\n",
    "                        pos_index[word][doc_id][\"title\"] = [pos]\n",
    "                    else:\n",
    "                        pos_index[word][doc_id][\"title\"].append(pos)\n",
    "                else:\n",
    "                    pos_index[word][doc_id] = {\"title\": [pos]}\n",
    "\n",
    "        for pos, word in enumerate(text):\n",
    "            if word not in pos_index:\n",
    "                pos_index[word] = {doc_id : {\"text\": [pos]}}\n",
    "            else:\n",
    "                if doc_id in pos_index[word].keys():\n",
    "                    if \"text\" not in pos_index[word][doc_id].keys():\n",
    "                        pos_index[word][doc_id][\"text\"] = [pos]\n",
    "                    else:\n",
    "                        pos_index[word][doc_id][\"text\"].append(pos)\n",
    "                else:\n",
    "                    pos_index[word][doc_id] = {\"text\": [pos]}\n",
    "\n",
    "\n",
    "    a = 0\n",
    "    return pos_index, doc_ids\n",
    "\n",
    "pos_index, docs = construct_positional_indexes('data\\Persian.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51VMBVg3JneM"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای مشاهده \n",
    "posting list\n",
    "یک کلمه و جایگاه‌های کلمه در هر بخش سند (عنوان و متن) است. تابع\n",
    "get_posting_list\n",
    "با گرفتن\n",
    "word\n",
    "به عنوان کلمه ورودی، یک دیکشنری به عنوان خروجی بر می‌گرداند که کلید‌های دیکشنری شناسه سند‌هایی است که کلمه در آن وجود داشته‌است.\n",
    "    برای هر شناسه سند آمده در کلید‌های دیکشنری، یک دیکشنری به عنوان مقدار وجود خواهد داشت که کلید‌های آن می‌تواند \n",
    "title\n",
    "و\n",
    "text\n",
    "باشد که جایگاه‌های آمدن کلمه در بخش‌های عنوان و متن به صورت لیست به عنوان مقدار هر یک از این کلید‌ها می‌آید. به طور مثال اگر یک کلمه مثل «سلام» در سند‌۱۰ در جایگاه ۲ عنوان و جایگاه‌های ۴ و ۸ متن و در سند ۲۹ در جایگاه ۱۹ متن آمده باشد دیکشنری به صورت آمده در قطعه کد زیر خواهد بود\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLnvKnQ0JneN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3016': {'text': [5,\n",
       "   41,\n",
       "   47,\n",
       "   52,\n",
       "   55,\n",
       "   66,\n",
       "   79,\n",
       "   83,\n",
       "   105,\n",
       "   127,\n",
       "   135,\n",
       "   220,\n",
       "   225,\n",
       "   230,\n",
       "   256,\n",
       "   307,\n",
       "   327,\n",
       "   385,\n",
       "   392,\n",
       "   401,\n",
       "   411,\n",
       "   416,\n",
       "   424,\n",
       "   470,\n",
       "   498,\n",
       "   663,\n",
       "   665,\n",
       "   685,\n",
       "   708,\n",
       "   823,\n",
       "   831,\n",
       "   837],\n",
       "  'title': [1]},\n",
       " '3039': {'text': [8631]},\n",
       " '3041': {'text': [119, 299, 321]},\n",
       " '3760': {'text': [3590]},\n",
       " '4337': {'text': [2930, 2945]},\n",
       " '4400': {'text': [7042]},\n",
       " '4595': {'text': [171, 248, 283, 497, 1543]},\n",
       " '7133': {'text': [47, 74, 101, 139, 250, 259, 790, 801, 1802]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_posting_list(word):\n",
    "    posting_list = {}\n",
    "    if word in pos_index.keys():\n",
    "        posting_list = pos_index[word]\n",
    "    # TODO\n",
    "    return posting_list\n",
    "\n",
    "\n",
    "get_posting_list('مهاباد')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "__qCYhAsJneS"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "(*امتیازی*)\n",
    "این بخش برای مشاهده تمام کلماتی است که دارای یک دوحرفی خاص درون خود هستند. تابع \n",
    "get_words_with_bigram\n",
    "یک ورودی به عنوان\n",
    "bigram\n",
    "می‌گیرد و تمام کلماتی را که دارای این دو حرفی هستند به عنوان خروجی بر می‌گرداند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6qxX9KAJneT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3014',\n",
       " '3016',\n",
       " '3017',\n",
       " '3021',\n",
       " '3022',\n",
       " '3023',\n",
       " '3026',\n",
       " '3027',\n",
       " '3029',\n",
       " '3030',\n",
       " '3033',\n",
       " '3036',\n",
       " '3037',\n",
       " '3039',\n",
       " '3041',\n",
       " '3043',\n",
       " '3046',\n",
       " '3047',\n",
       " '3049',\n",
       " '3055',\n",
       " '3056',\n",
       " '3058',\n",
       " '3059',\n",
       " '3060',\n",
       " '3061',\n",
       " '3065',\n",
       " '3068',\n",
       " '3069',\n",
       " '3070',\n",
       " '3071',\n",
       " '3072',\n",
       " '3073',\n",
       " '3074',\n",
       " '3076',\n",
       " '3091',\n",
       " '3095',\n",
       " '3098',\n",
       " '3099',\n",
       " '3100',\n",
       " '3101',\n",
       " '3103',\n",
       " '3111',\n",
       " '3117',\n",
       " '3118',\n",
       " '3119',\n",
       " '3120',\n",
       " '3121',\n",
       " '3128',\n",
       " '3129',\n",
       " '3130',\n",
       " '3141',\n",
       " '3157',\n",
       " '3179',\n",
       " '3197',\n",
       " '3199',\n",
       " '3205',\n",
       " '3217',\n",
       " '3219',\n",
       " '3220',\n",
       " '3227',\n",
       " '3229',\n",
       " '3233',\n",
       " '3243',\n",
       " '3248',\n",
       " '3251',\n",
       " '3252',\n",
       " '3260',\n",
       " '3261',\n",
       " '3263',\n",
       " '3273',\n",
       " '3274',\n",
       " '3276',\n",
       " '3277',\n",
       " '3280',\n",
       " '3282',\n",
       " '3283',\n",
       " '3284',\n",
       " '3285',\n",
       " '3286',\n",
       " '3287',\n",
       " '3288',\n",
       " '3289',\n",
       " '3290',\n",
       " '3291',\n",
       " '3292',\n",
       " '3293',\n",
       " '3294',\n",
       " '3295',\n",
       " '3296',\n",
       " '3297',\n",
       " '3298',\n",
       " '3300',\n",
       " '3301',\n",
       " '3302',\n",
       " '3303',\n",
       " '3304',\n",
       " '3305',\n",
       " '3307',\n",
       " '3308',\n",
       " '3309',\n",
       " '3310',\n",
       " '3311',\n",
       " '3312',\n",
       " '3313',\n",
       " '3314',\n",
       " '3315',\n",
       " '3316',\n",
       " '3317',\n",
       " '3318',\n",
       " '3319',\n",
       " '3320',\n",
       " '3324',\n",
       " '3325',\n",
       " '3329',\n",
       " '3331',\n",
       " '3332',\n",
       " '3333',\n",
       " '3334',\n",
       " '3335',\n",
       " '3339',\n",
       " '3341',\n",
       " '3342',\n",
       " '3343',\n",
       " '3345',\n",
       " '3347',\n",
       " '3348',\n",
       " '3359',\n",
       " '3360',\n",
       " '3361',\n",
       " '3362',\n",
       " '3367',\n",
       " '3373',\n",
       " '3376',\n",
       " '3377',\n",
       " '3380',\n",
       " '3381',\n",
       " '3385',\n",
       " '3394',\n",
       " '3397',\n",
       " '3400',\n",
       " '3402',\n",
       " '3404',\n",
       " '3406',\n",
       " '3409',\n",
       " '3410',\n",
       " '3411',\n",
       " '3413',\n",
       " '3414',\n",
       " '3415',\n",
       " '3416',\n",
       " '3429',\n",
       " '3433',\n",
       " '3443',\n",
       " '3456',\n",
       " '3458',\n",
       " '3463',\n",
       " '3507',\n",
       " '3521',\n",
       " '3539',\n",
       " '3576',\n",
       " '3580',\n",
       " '3582',\n",
       " '3653',\n",
       " '3654',\n",
       " '3656',\n",
       " '3658',\n",
       " '3664',\n",
       " '3665',\n",
       " '3666',\n",
       " '3667',\n",
       " '3670',\n",
       " '3671',\n",
       " '3672',\n",
       " '3680',\n",
       " '3681',\n",
       " '3690',\n",
       " '3695',\n",
       " '3699',\n",
       " '3702',\n",
       " '3711',\n",
       " '3716',\n",
       " '3719',\n",
       " '3721',\n",
       " '3727',\n",
       " '3730',\n",
       " '3733',\n",
       " '3738',\n",
       " '3739',\n",
       " '3740',\n",
       " '3741',\n",
       " '3742',\n",
       " '3743',\n",
       " '3744',\n",
       " '3747',\n",
       " '3750',\n",
       " '3751',\n",
       " '3752',\n",
       " '3758',\n",
       " '3760',\n",
       " '3761',\n",
       " '3776',\n",
       " '3777',\n",
       " '3778',\n",
       " '3780',\n",
       " '3782',\n",
       " '3784',\n",
       " '3785',\n",
       " '3787',\n",
       " '3790',\n",
       " '3797',\n",
       " '3798',\n",
       " '3800',\n",
       " '3801',\n",
       " '3802',\n",
       " '3803',\n",
       " '3804',\n",
       " '3809',\n",
       " '3814',\n",
       " '3820',\n",
       " '3821',\n",
       " '3826',\n",
       " '3829',\n",
       " '3835',\n",
       " '3836',\n",
       " '3838',\n",
       " '3841',\n",
       " '3842',\n",
       " '3843',\n",
       " '3844',\n",
       " '3845',\n",
       " '3847',\n",
       " '3849',\n",
       " '3851',\n",
       " '3852',\n",
       " '3853',\n",
       " '3854',\n",
       " '3855',\n",
       " '3860',\n",
       " '3862',\n",
       " '3865',\n",
       " '3866',\n",
       " '3868',\n",
       " '3871',\n",
       " '3872',\n",
       " '3874',\n",
       " '3879',\n",
       " '3880',\n",
       " '3881',\n",
       " '3884',\n",
       " '3885',\n",
       " '3886',\n",
       " '3889',\n",
       " '3894',\n",
       " '3897',\n",
       " '3901',\n",
       " '3903',\n",
       " '3904',\n",
       " '3905',\n",
       " '3906',\n",
       " '3907',\n",
       " '3908',\n",
       " '3910',\n",
       " '3911',\n",
       " '3912',\n",
       " '3915',\n",
       " '3916',\n",
       " '3918',\n",
       " '3919',\n",
       " '3920',\n",
       " '3921',\n",
       " '3923',\n",
       " '3925',\n",
       " '3926',\n",
       " '3929',\n",
       " '3930',\n",
       " '3931',\n",
       " '3932',\n",
       " '3933',\n",
       " '3934',\n",
       " '3936',\n",
       " '3938',\n",
       " '3939',\n",
       " '3941',\n",
       " '3944',\n",
       " '3945',\n",
       " '3946',\n",
       " '3951',\n",
       " '3954',\n",
       " '3955',\n",
       " '3956',\n",
       " '3961',\n",
       " '3965',\n",
       " '3967',\n",
       " '3968',\n",
       " '3971',\n",
       " '3975',\n",
       " '3978',\n",
       " '3979',\n",
       " '3980',\n",
       " '3983',\n",
       " '3986',\n",
       " '3987',\n",
       " '3989',\n",
       " '3990',\n",
       " '3992',\n",
       " '3993',\n",
       " '3995',\n",
       " '3996',\n",
       " '3997',\n",
       " '4002',\n",
       " '4007',\n",
       " '4008',\n",
       " '4009',\n",
       " '4017',\n",
       " '4018',\n",
       " '4021',\n",
       " '4023',\n",
       " '4024',\n",
       " '4025',\n",
       " '4026',\n",
       " '4027',\n",
       " '4028',\n",
       " '4030',\n",
       " '4032',\n",
       " '4034',\n",
       " '4036',\n",
       " '4037',\n",
       " '4039',\n",
       " '4040',\n",
       " '4041',\n",
       " '4042',\n",
       " '4048',\n",
       " '4049',\n",
       " '4050',\n",
       " '4051',\n",
       " '4054',\n",
       " '4055',\n",
       " '4056',\n",
       " '4057',\n",
       " '4058',\n",
       " '4059',\n",
       " '4062',\n",
       " '4065',\n",
       " '4066',\n",
       " '4069',\n",
       " '4070',\n",
       " '4072',\n",
       " '4074',\n",
       " '4080',\n",
       " '4081',\n",
       " '4082',\n",
       " '4086',\n",
       " '4088',\n",
       " '4094',\n",
       " '4100',\n",
       " '4102',\n",
       " '4103',\n",
       " '4104',\n",
       " '4105',\n",
       " '4106',\n",
       " '4107',\n",
       " '4109',\n",
       " '4110',\n",
       " '4111',\n",
       " '4112',\n",
       " '4113',\n",
       " '4114',\n",
       " '4115',\n",
       " '4118',\n",
       " '4120',\n",
       " '4131',\n",
       " '4133',\n",
       " '4135',\n",
       " '4139',\n",
       " '4140',\n",
       " '4141',\n",
       " '4142',\n",
       " '4143',\n",
       " '4144',\n",
       " '4145',\n",
       " '4146',\n",
       " '4147',\n",
       " '4148',\n",
       " '4149',\n",
       " '4151',\n",
       " '4152',\n",
       " '4153',\n",
       " '4154',\n",
       " '4155',\n",
       " '4156',\n",
       " '4157',\n",
       " '4158',\n",
       " '4159',\n",
       " '4160',\n",
       " '4161',\n",
       " '4162',\n",
       " '4163',\n",
       " '4164',\n",
       " '4165',\n",
       " '4166',\n",
       " '4167',\n",
       " '4168',\n",
       " '4169',\n",
       " '4172',\n",
       " '4183',\n",
       " '4190',\n",
       " '4191',\n",
       " '4204',\n",
       " '4205',\n",
       " '4206',\n",
       " '4207',\n",
       " '4208',\n",
       " '4209',\n",
       " '4210',\n",
       " '4211',\n",
       " '4212',\n",
       " '4213',\n",
       " '4214',\n",
       " '4215',\n",
       " '4216',\n",
       " '4217',\n",
       " '4218',\n",
       " '4219',\n",
       " '4220',\n",
       " '4221',\n",
       " '4222',\n",
       " '4223',\n",
       " '4224',\n",
       " '4225',\n",
       " '4226',\n",
       " '4227',\n",
       " '4228',\n",
       " '4229',\n",
       " '4230',\n",
       " '4231',\n",
       " '4240',\n",
       " '4247',\n",
       " '4248',\n",
       " '4254',\n",
       " '4255',\n",
       " '4256',\n",
       " '4257',\n",
       " '4259',\n",
       " '4263',\n",
       " '4269',\n",
       " '4274',\n",
       " '4275',\n",
       " '4281',\n",
       " '4282',\n",
       " '4283',\n",
       " '4284',\n",
       " '4285',\n",
       " '4286',\n",
       " '4287',\n",
       " '4288',\n",
       " '4289',\n",
       " '4290',\n",
       " '4291',\n",
       " '4292',\n",
       " '4293',\n",
       " '4294',\n",
       " '4295',\n",
       " '4296',\n",
       " '4297',\n",
       " '4299',\n",
       " '4300',\n",
       " '4301',\n",
       " '4306',\n",
       " '4313',\n",
       " '4321',\n",
       " '4323',\n",
       " '4329',\n",
       " '4332',\n",
       " '4335',\n",
       " '4337',\n",
       " '4339',\n",
       " '4363',\n",
       " '4368',\n",
       " '4375',\n",
       " '4376',\n",
       " '4377',\n",
       " '4379',\n",
       " '4382',\n",
       " '4383',\n",
       " '4385',\n",
       " '4388',\n",
       " '4391',\n",
       " '4394',\n",
       " '4395',\n",
       " '4396',\n",
       " '4397',\n",
       " '4398',\n",
       " '4399',\n",
       " '4400',\n",
       " '4401',\n",
       " '4402',\n",
       " '4404',\n",
       " '4406',\n",
       " '4408',\n",
       " '4409',\n",
       " '4410',\n",
       " '4415',\n",
       " '4416',\n",
       " '4417',\n",
       " '4422',\n",
       " '4423',\n",
       " '4425',\n",
       " '4427',\n",
       " '4438',\n",
       " '4439',\n",
       " '4443',\n",
       " '4444',\n",
       " '4446',\n",
       " '4447',\n",
       " '4451',\n",
       " '4455',\n",
       " '4459',\n",
       " '4460',\n",
       " '4462',\n",
       " '4463',\n",
       " '4466',\n",
       " '4468',\n",
       " '4470',\n",
       " '4471',\n",
       " '4473',\n",
       " '4475',\n",
       " '4476',\n",
       " '4484',\n",
       " '4485',\n",
       " '4486',\n",
       " '4487',\n",
       " '4488',\n",
       " '4489',\n",
       " '4492',\n",
       " '4493',\n",
       " '4497',\n",
       " '4503',\n",
       " '4505',\n",
       " '4506',\n",
       " '4508',\n",
       " '4509',\n",
       " '4512',\n",
       " '4513',\n",
       " '4514',\n",
       " '4515',\n",
       " '4516',\n",
       " '4517',\n",
       " '4518',\n",
       " '4519',\n",
       " '4524',\n",
       " '4525',\n",
       " '4526',\n",
       " '4527',\n",
       " '4528',\n",
       " '4530',\n",
       " '4531',\n",
       " '4532',\n",
       " '4533',\n",
       " '4534',\n",
       " '4536',\n",
       " '4537',\n",
       " '4539',\n",
       " '4540',\n",
       " '4541',\n",
       " '4542',\n",
       " '4543',\n",
       " '4544',\n",
       " '4545',\n",
       " '4546',\n",
       " '4547',\n",
       " '4548',\n",
       " '4549',\n",
       " '4550',\n",
       " '4551',\n",
       " '4554',\n",
       " '4555',\n",
       " '4556',\n",
       " '4557',\n",
       " '4560',\n",
       " '4561',\n",
       " '4563',\n",
       " '4564',\n",
       " '4568',\n",
       " '4570',\n",
       " '4571',\n",
       " '4573',\n",
       " '4576',\n",
       " '4577',\n",
       " '4582',\n",
       " '4586',\n",
       " '4589',\n",
       " '4593',\n",
       " '4595',\n",
       " '4597',\n",
       " '4598',\n",
       " '4601',\n",
       " '4603',\n",
       " '4605',\n",
       " '4609',\n",
       " '4611',\n",
       " '4612',\n",
       " '4621',\n",
       " '4624',\n",
       " '4625',\n",
       " '4627',\n",
       " '4628',\n",
       " '4629',\n",
       " '4636',\n",
       " '4637',\n",
       " '4638',\n",
       " '4646',\n",
       " '4650',\n",
       " '4655',\n",
       " '4656',\n",
       " '4658',\n",
       " '4659',\n",
       " '4662',\n",
       " '4665',\n",
       " '4666',\n",
       " '4668',\n",
       " '4669',\n",
       " '4670',\n",
       " '4671',\n",
       " '4676',\n",
       " '4677',\n",
       " '4678',\n",
       " '4679',\n",
       " '4680',\n",
       " '4681',\n",
       " '4683',\n",
       " '4684',\n",
       " '4686',\n",
       " '4687',\n",
       " '4690',\n",
       " '4691',\n",
       " '4707',\n",
       " '4710',\n",
       " '4718',\n",
       " '4719',\n",
       " '4720',\n",
       " '4725',\n",
       " '4726',\n",
       " '4730',\n",
       " '4731',\n",
       " '4732',\n",
       " '4734',\n",
       " '4736',\n",
       " '4737',\n",
       " '4742',\n",
       " '4743',\n",
       " '4745',\n",
       " '4746',\n",
       " '4748',\n",
       " '4752',\n",
       " '4755',\n",
       " '4757',\n",
       " '4758',\n",
       " '4760',\n",
       " '4761',\n",
       " '4762',\n",
       " '4763',\n",
       " '4765',\n",
       " '4766',\n",
       " '4777',\n",
       " '4778',\n",
       " '4779',\n",
       " '4780',\n",
       " '4781',\n",
       " '4784',\n",
       " '4799',\n",
       " '4805',\n",
       " '4811',\n",
       " '4815',\n",
       " '4816',\n",
       " '4818',\n",
       " '4819',\n",
       " '4820',\n",
       " '4824',\n",
       " '4831',\n",
       " '4833',\n",
       " '4838',\n",
       " '4839',\n",
       " '4840',\n",
       " '4843',\n",
       " '4846',\n",
       " '4847',\n",
       " '4849',\n",
       " '4856',\n",
       " '4858',\n",
       " '4864',\n",
       " '4865',\n",
       " '4867',\n",
       " '4872',\n",
       " '4874',\n",
       " '4876',\n",
       " '4878',\n",
       " '4893',\n",
       " '4896',\n",
       " '4897',\n",
       " '4898',\n",
       " '4899',\n",
       " '4901',\n",
       " '4903',\n",
       " '4905',\n",
       " '4906',\n",
       " '4907',\n",
       " '4908',\n",
       " '4909',\n",
       " '4910',\n",
       " '4911',\n",
       " '4912',\n",
       " '4914',\n",
       " '4916',\n",
       " '4918',\n",
       " '4919',\n",
       " '4929',\n",
       " '4938',\n",
       " '4940',\n",
       " '4941',\n",
       " '4942',\n",
       " '4953',\n",
       " '4955',\n",
       " '4986',\n",
       " '4990',\n",
       " '4992',\n",
       " '4993',\n",
       " '4996',\n",
       " '4997',\n",
       " '4998',\n",
       " '5010',\n",
       " '5011',\n",
       " '5012',\n",
       " '5028',\n",
       " '5031',\n",
       " '5039',\n",
       " '5048',\n",
       " '5049',\n",
       " '5050',\n",
       " '5052',\n",
       " '5053',\n",
       " '5067',\n",
       " '5071',\n",
       " '5122',\n",
       " '5126',\n",
       " '5134',\n",
       " '5141',\n",
       " '5147',\n",
       " '5152',\n",
       " '5153',\n",
       " '5173',\n",
       " '5174',\n",
       " '5185',\n",
       " '5187',\n",
       " '5189',\n",
       " '5192',\n",
       " '5195',\n",
       " '5201',\n",
       " '5203',\n",
       " '5206',\n",
       " '5236',\n",
       " '5237',\n",
       " '5238',\n",
       " '5240',\n",
       " '5241',\n",
       " '5252',\n",
       " '5255',\n",
       " '5256',\n",
       " '5257',\n",
       " '5260',\n",
       " '5261',\n",
       " '5262',\n",
       " '5263',\n",
       " '5264',\n",
       " '5266',\n",
       " '5268',\n",
       " '5269',\n",
       " '5275',\n",
       " '5277',\n",
       " '5281',\n",
       " '5282',\n",
       " '5284',\n",
       " '5285',\n",
       " '5287',\n",
       " '5288',\n",
       " '5293',\n",
       " '5294',\n",
       " '5296',\n",
       " '5299',\n",
       " '5301',\n",
       " '5303',\n",
       " '5304',\n",
       " '5306',\n",
       " '5307',\n",
       " '5308',\n",
       " '5309',\n",
       " '5310',\n",
       " '5311',\n",
       " '5312',\n",
       " '5318',\n",
       " '5319',\n",
       " '5320',\n",
       " '5342',\n",
       " '5346',\n",
       " '5352',\n",
       " '5353',\n",
       " '5354',\n",
       " '5355',\n",
       " '5357',\n",
       " '5359',\n",
       " '5360',\n",
       " '5362',\n",
       " '5364',\n",
       " '5366',\n",
       " '5369',\n",
       " '5370',\n",
       " '5371',\n",
       " '5372',\n",
       " '5375',\n",
       " '5378',\n",
       " '5380',\n",
       " '5381',\n",
       " '5382',\n",
       " '5383',\n",
       " '5386',\n",
       " '5390',\n",
       " '5393',\n",
       " '5396',\n",
       " '5398',\n",
       " '5399',\n",
       " '5400',\n",
       " '5402',\n",
       " '5403',\n",
       " '5408',\n",
       " '5409',\n",
       " '5410',\n",
       " '5411',\n",
       " '5414',\n",
       " '5415',\n",
       " '5416',\n",
       " '5422',\n",
       " '5428',\n",
       " '5429',\n",
       " '5430',\n",
       " '5440',\n",
       " '5444',\n",
       " '5445',\n",
       " '5446',\n",
       " '5448',\n",
       " '5449',\n",
       " '5450',\n",
       " '5451',\n",
       " '5453',\n",
       " '5454',\n",
       " '5455',\n",
       " '5456',\n",
       " '5458',\n",
       " '5459',\n",
       " '5460',\n",
       " '5462',\n",
       " '5465',\n",
       " '5466',\n",
       " '5474',\n",
       " '5478',\n",
       " '5481',\n",
       " '5484',\n",
       " '5486',\n",
       " '5490',\n",
       " '5492',\n",
       " '5494',\n",
       " '5495',\n",
       " '5496',\n",
       " '5497',\n",
       " '5498',\n",
       " '5499',\n",
       " '5501',\n",
       " '5503',\n",
       " '5504',\n",
       " '5506',\n",
       " '5508',\n",
       " '5509',\n",
       " '5510',\n",
       " '5512',\n",
       " '5517',\n",
       " '5519',\n",
       " '5522',\n",
       " '5523',\n",
       " '5524',\n",
       " '5525',\n",
       " '5526',\n",
       " '5527',\n",
       " '5529',\n",
       " '5533',\n",
       " '5534',\n",
       " '5542',\n",
       " '5549',\n",
       " '5551',\n",
       " '5553',\n",
       " '5554',\n",
       " '5560',\n",
       " '5568',\n",
       " '5569',\n",
       " '5571',\n",
       " '5572',\n",
       " '5573',\n",
       " '5576',\n",
       " '5579',\n",
       " '5580',\n",
       " '5581',\n",
       " '5582',\n",
       " '5583',\n",
       " '5584',\n",
       " '5585',\n",
       " '5586',\n",
       " '5587',\n",
       " '5588',\n",
       " '5589',\n",
       " '5590',\n",
       " '5591',\n",
       " '5592',\n",
       " '5593',\n",
       " '5594',\n",
       " '5595',\n",
       " '5596',\n",
       " '5597',\n",
       " '5598',\n",
       " '5600',\n",
       " '5601',\n",
       " '5602',\n",
       " '5603',\n",
       " '5604',\n",
       " '5605',\n",
       " '5607',\n",
       " '5608',\n",
       " '5610',\n",
       " '5611',\n",
       " '5612',\n",
       " '5613',\n",
       " '5615',\n",
       " '5616',\n",
       " '5619',\n",
       " '5620',\n",
       " '5636',\n",
       " '5644',\n",
       " '5660',\n",
       " '5665',\n",
       " '5668',\n",
       " '5669',\n",
       " '5678',\n",
       " '5679',\n",
       " '5680',\n",
       " '5681',\n",
       " '5683',\n",
       " '5686',\n",
       " '5690',\n",
       " '5693',\n",
       " '5694',\n",
       " '5696',\n",
       " '5698',\n",
       " '5700',\n",
       " '5701',\n",
       " '5702',\n",
       " '5703',\n",
       " '5704',\n",
       " '5705',\n",
       " '5707',\n",
       " '5708',\n",
       " '5709',\n",
       " '5710',\n",
       " '5714',\n",
       " '5720',\n",
       " '5721',\n",
       " '5723',\n",
       " '5724',\n",
       " '5725',\n",
       " '5727',\n",
       " '5728',\n",
       " '5729',\n",
       " '5731',\n",
       " '5735',\n",
       " '5739',\n",
       " '5740',\n",
       " '5746',\n",
       " '5748',\n",
       " '5749',\n",
       " '5759',\n",
       " '5763',\n",
       " '5765',\n",
       " '5766',\n",
       " '5767',\n",
       " '5772',\n",
       " '5773',\n",
       " '5776',\n",
       " '5777',\n",
       " '5779',\n",
       " '5781',\n",
       " '5784',\n",
       " '5786',\n",
       " '5787',\n",
       " '5793',\n",
       " '5805',\n",
       " ...]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_words_with_bigram(bigram):\n",
    "    words = ['سلام', 'کلام', 'اصلاح', 'اطلاعات']\n",
    "    # TODO\n",
    "    return words\n",
    "\n",
    "get_words_with_bigram('لا')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Rz7JnPLJneY"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای اضافه کردن یک سند به نمایه‌ها است.\n",
    "تابع\n",
    "add_document_to_indexes\n",
    "با گرفتن مسیر پوشه داده‌ها و یک شناسه،\n",
    "در صورت نبود آن سند در نمایه‌ها، آن را به نمایه‌ها اضافه می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3V8mUFHJneZ"
   },
   "outputs": [],
   "source": [
    "def add_document_to_indexes(docs_path, pos_index,docs):\n",
    "    tree = ET.parse(docs_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    doc_id = root.findall(\n",
    "        \"./{http://www.mediawiki.org/xml/export-0.10/}page/{http://www.mediawiki.org/xml/export-0.10/}id\")[0].text\n",
    "    title = root.findall(\n",
    "        \"./{http://www.mediawiki.org/xml/export-0.10/}page/{http://www.mediawiki.org/xml/export-0.10/}title\")[0]\n",
    "    text = root.findall(\n",
    "        \"./{http://www.mediawiki.org/xml/export-0.10/}page/{http://www.mediawiki.org/xml/export-0.10/}revision/{http://www.mediawiki.org/xml/export-0.10/}text\")[0]\n",
    "    title_text_pair = {doc_id : {\"title\": title.text, \"text\": text.text}}\n",
    "\n",
    "    prepared_title = prepare_text(title_text_pair[doc_id][\"title\"])\n",
    "    prepared_text = prepare_text(title_text_pair[doc_id][\"text\"])\n",
    "    for pos, word in enumerate(prepared_title):\n",
    "        if word not in pos_index:\n",
    "            pos_index[word] = {doc_id: {\"title\": [pos]}}\n",
    "        else:\n",
    "            if doc_id in pos_index[word].keys():\n",
    "                if \"title\" not in pos_index[word][doc_id].keys():\n",
    "                    pos_index[word][doc_id][\"title\"] = [pos]\n",
    "                else:\n",
    "                    pos_index[word][doc_id][\"title\"].append(pos)\n",
    "                    pos_index[word][doc_id][\"title\"] = sorted(pos_index[word][doc_id][\"title\"])\n",
    "\n",
    "            else:\n",
    "                pos_index[word][doc_id] = {\"title\": [pos]}\n",
    "\n",
    "    for pos, word in enumerate(prepared_text):\n",
    "        if word not in pos_index:\n",
    "            pos_index[word] = {doc_id: {\"text\": [pos]}}\n",
    "        else:\n",
    "            if doc_id in pos_index[word].keys():\n",
    "                if \"text\" not in pos_index[word][doc_id].keys():\n",
    "                    pos_index[word][doc_id][\"text\"] = [pos]\n",
    "                else:\n",
    "                    pos_index[word][doc_id][\"text\"].append(pos)\n",
    "                    pos_index[word][doc_id][\"text\"] = sorted(pos_index[word][doc_id][\"text\"])\n",
    "            else:\n",
    "                pos_index[word][doc_id] = {\"text\": [pos]}\n",
    "\n",
    "    \n",
    "    docs.append(doc_id)\n",
    "    docs = sorted(docs)\n",
    "    N = len(docs)\n",
    "    return pos_index, docs, N\n",
    "\n",
    "\n",
    "\n",
    "pos_index, docs, N = add_document_to_indexes('data\\\\new_doc.xml', pos_index,docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fI62QI7FJnec"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای حذف کردن یک سند از نمایه است.\n",
    "تابع\n",
    "delete_document_from_indexes\n",
    "با گرفتن مسیر پوشه داده‌ها و یک شناسه سند، آن سند را از نمایه‌ها حذف می‌کند.\n",
    "در صورتی که پس از حذف یک سند، \n",
    "یک کلمه دیگر در بین محتوای سند‌ها وجود نداشته‌باشد، آن کلمه باید از دیکشنری نمایه‌ی اصلی \n",
    "به طور کامل حذف شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJ_8rPAxJneg"
   },
   "outputs": [],
   "source": [
    "def delete_document_from_indexes(docs_path, docs):\n",
    "    tree = ET.parse(docs_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    doc_id = root.findall(\n",
    "        \"./{http://www.mediawiki.org/xml/export-0.10/}page/{http://www.mediawiki.org/xml/export-0.10/}id\")[0].text\n",
    "    title = root.findall(\n",
    "        \"./{http://www.mediawiki.org/xml/export-0.10/}page/{http://www.mediawiki.org/xml/export-0.10/}title\")[0]\n",
    "    text = root.findall(\n",
    "        \"./{http://www.mediawiki.org/xml/export-0.10/}page/{http://www.mediawiki.org/xml/export-0.10/}revision/{http://www.mediawiki.org/xml/export-0.10/}text\")[\n",
    "        0]\n",
    "    title_text_pair = {doc_id: {\"title\": title.text, \"text\": text.text}}\n",
    "\n",
    "    prepared_title = prepare_text(title_text_pair[doc_id][\"title\"])\n",
    "    prepared_text = prepare_text(title_text_pair[doc_id][\"text\"])\n",
    "    for pos, word in enumerate(prepared_title):\n",
    "        if word in pos_index.keys():\n",
    "            if doc_id in pos_index[word].keys():\n",
    "                pos_index[word].pop(doc_id, None)\n",
    "\n",
    "            if len(pos_index[word]) == 0:\n",
    "                pos_index.pop(word, None)\n",
    "\n",
    "    for pos, word in enumerate(prepared_text):\n",
    "        if word in pos_index.keys():\n",
    "            if doc_id in pos_index[word].keys():\n",
    "                pos_index[word].pop(doc_id, None)\n",
    "\n",
    "            if len(pos_index[word]) == 0:\n",
    "                pos_index.pop(word, None)\n",
    "    docs.remove(doc_id)\n",
    "    N = len(docs)\n",
    "    \n",
    "    return pos_index, docs, N\n",
    "\n",
    "\n",
    "pos_index, docs, N = delete_document_from_indexes('data\\\\new_doc.xml', docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3074': {'text': [210]},\n",
       " '3119': {'text': [2683]},\n",
       " '3415': {'text': [2283,\n",
       "   2434,\n",
       "   2452,\n",
       "   2493,\n",
       "   2505,\n",
       "   2521,\n",
       "   2616,\n",
       "   2939,\n",
       "   3050,\n",
       "   3085,\n",
       "   3113,\n",
       "   3119,\n",
       "   3134,\n",
       "   3165,\n",
       "   3281,\n",
       "   3332,\n",
       "   3364,\n",
       "   3547]},\n",
       " '3429': {'text': [2090, 2120, 2132]},\n",
       " '4162': {'text': [204]},\n",
       " '4259': {'text': [768]},\n",
       " '4460': {'text': [4338]},\n",
       " '4573': {'text': [8101]},\n",
       " '4624': {'text': [426,\n",
       "   1046,\n",
       "   1118,\n",
       "   1914,\n",
       "   1969,\n",
       "   2104,\n",
       "   2120,\n",
       "   2150,\n",
       "   2160,\n",
       "   2278,\n",
       "   3049,\n",
       "   3053,\n",
       "   6516,\n",
       "   6521,\n",
       "   10146,\n",
       "   10945]},\n",
       " '4636': {'text': [1, 91, 105, 503, 509]},\n",
       " '4637': {'text': [175, 222, 248]},\n",
       " '4676': {'text': [0, 20, 55, 98, 205, 258, 580, 593, 611], 'title': [0]},\n",
       " '5237': {'text': [1,\n",
       "   158,\n",
       "   214,\n",
       "   245,\n",
       "   358,\n",
       "   553,\n",
       "   590,\n",
       "   711,\n",
       "   1242,\n",
       "   1782,\n",
       "   2392,\n",
       "   2483,\n",
       "   2543,\n",
       "   2637,\n",
       "   2698,\n",
       "   2795,\n",
       "   2803,\n",
       "   2806,\n",
       "   3344,\n",
       "   4455,\n",
       "   5053,\n",
       "   5354,\n",
       "   5374,\n",
       "   5562,\n",
       "   5745,\n",
       "   5750,\n",
       "   6038,\n",
       "   6053,\n",
       "   6058,\n",
       "   6067,\n",
       "   6098,\n",
       "   6100,\n",
       "   6615,\n",
       "   7078,\n",
       "   7362,\n",
       "   7807,\n",
       "   7816,\n",
       "   7827,\n",
       "   8048,\n",
       "   8248,\n",
       "   8261,\n",
       "   8463,\n",
       "   8561,\n",
       "   8599,\n",
       "   8905,\n",
       "   8963,\n",
       "   9041,\n",
       "   9410,\n",
       "   9420,\n",
       "   9888,\n",
       "   10132,\n",
       "   11090,\n",
       "   14145,\n",
       "   14488,\n",
       "   14504,\n",
       "   14750,\n",
       "   14759,\n",
       "   15238,\n",
       "   16286,\n",
       "   16452,\n",
       "   16467,\n",
       "   16559,\n",
       "   16586,\n",
       "   16597,\n",
       "   17743,\n",
       "   17975]},\n",
       " '5256': {'text': [1742]},\n",
       " '5293': {'text': [4297]},\n",
       " '6163': {'text': [771]},\n",
       " '6187': {'text': [2177]},\n",
       " '6369': {'text': [530]},\n",
       " '6459': {'text': [11265]},\n",
       " '6466': {'text': [123]},\n",
       " '6489': {'text': [240, 426]},\n",
       " '6491': {'text': [590]},\n",
       " '6507': {'text': [222, 232, 255]},\n",
       " '6629': {'text': [5100, 5116, 5158]},\n",
       " '6694': {'text': [8950]},\n",
       " '6752': {'text': [8782]},\n",
       " '6815': {'text': [59]},\n",
       " '6838': {'text': [1845]},\n",
       " '6847': {'text': [1497, 1510, 1520]},\n",
       " '6966': {'text': [1876]},\n",
       " '7013': {'text': [3699, 3832]},\n",
       " '7051': {'text': [111, 141, 674, 719, 998, 1093, 1298, 1364]},\n",
       " '7060': {'text': [1179, 1462]},\n",
       " '7141': {'text': [512]},\n",
       " '7142': {'text': [1664]},\n",
       " '7153': {'text': [1569]},\n",
       " '7156': {'text': [791, 958, 2288]}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_index[\"ذر\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QL-19qvrJnek"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای ذخیره‌سازی نمایه‌ی اول است\n",
    "و نیازی به ذخیره‌سازی نمایه \n",
    "Bigram نیست \n",
    ".\n",
    "تابع \n",
    "save_index\n",
    "گرفتن مسیر فایل ذخیره کردن نمایه با نام \n",
    "destination\n",
    "نمایه ساخته‌شده را در این مسیر ذخیره می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UestOXpOJnel"
   },
   "outputs": [],
   "source": [
    "def save_index(destination):\n",
    "    with open(destination + \"backup_indexes.txt\", 'w') as outfile:\n",
    "        json.dump(pos_index, outfile)\n",
    "    str_docs = \"\"\n",
    "    for d in docs:\n",
    "        str_docs = str_docs + d + \" \"\n",
    "    with open(destination + \"docs.txt\", \"w\") as outfile:\n",
    "        outfile.write(str_docs)\n",
    "        \n",
    "\n",
    "save_index('storage\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPL51GrmJnep"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش برای بارگذاری نمایه از یک فایل است. تابع \n",
    "load_index\n",
    "با گرفتن مسیر فایل ذخیره شده نمایه با نام \n",
    "source\n",
    "نمایه را از این فایل بارگذاری می‌کند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahonOMFMJneq"
   },
   "outputs": [],
   "source": [
    "def load_index(source):\n",
    "    with open(source + \"backup_indexes.txt\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    with open(source + \"docs.txt\") as outfile:\n",
    "        docs = outfile.read()\n",
    "    \n",
    "    return data, docs\n",
    "\n",
    "pos_index, docs = load_index('storage\\\\')\n",
    "docs = docs.split(\" \")\n",
    "docs = docs[:len(docs) - 1]\n",
    "N = len(docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5JeSxd2Jnet"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (40 نمره) بخش سوم: جستجو وبازیابی اسناد</div>\n",
    "</font>\n",
    "<hr>\n",
    "در این قسمت لازم است تا پرسمانی که از کاربر گرفته می‌شود در مجموعه اسناد نمایه شده، جستجو شود. جستجو به دو صورت بازیابی ترتیب دار در فضای برداری و بازیابی دقیق عبارت است. جستجو باید هم در عنوان سند صورت بگیرد هم در متن آن و در نهایت ترتیب اسناد بازگردانده شده بر اساس امتیازی‌ است که از جمع وزن‌دار امتیاز جست‌وجو در عنوان و جست‌وجو در متن به دست آمده‌است.\n",
    "(*امتیازی*)\n",
    "همچنین گاهی ممکن است پرسمان ارائه شده حاوی غلط املایی باشد، در این صورت لازم است تا ابتدا پرسمان را اصلاح کنید و سپس جستجو انجام شود. \n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "(*امتیازی*)\n",
    "اصلاح پرسمان\n",
    "</font>\n",
    "<br>\n",
    "اصلاح پرسمان ورودی: ممکن است پرسمان ورودی\n",
    "کاربر غلط املایی داشته باشد؛ در چنین مواردی برای هر لغت از پرسمان ورودی که در نمایه موجود  نیست ابتدا نزدیکترین لغات موجود در نمایه \n",
    "bigram\n",
    "(با استفاده از معیار فاصله جاکارد) \n",
    "انتخاب شده و سپس\n",
    "بهترین آنها با معیار \n",
    "edit distance\n",
    "نسبت به کلمه اصلی، جایگزین می‌شود. در صورتی که چند لغت فاصله برابری از لغت مورد نظر داشته باشند، می‌توانید یکی از آنها را به دلخواه انتخاب کنید.\n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "بازیابی ترتیب دار در فضای برداری tf-idf به روشهای ltn-lnn و ltc-lnc\n",
    "</font>\n",
    "<br>\n",
    " در این بخش پرسمان به صورت یک پرسمان کلی مطرح می‌شود و جست‌وجوی یک پرسمان بر روی هر دو بخش عنوان و متن صورت می‌گیرد و سپس نتیجه بر اساس امتیاز وزن‌دار جست و جو بر روی این دو بخش به ترتیب برگردانده می‌شود. وزن امتیاز جست‌وجو در عنوان نسبت به وزن امتیاز جست‌وجو در متن باید به عنوان پارامتر ورودی قابل تنظیم باشد اما در حالت پیش‌فرض آن را ۲ در نظر می‌گیریم. \n",
    "<br>\n",
    "برای هر پرسمان، پس از مشخص شدن روش امتیازدهی به عنوان ورودی\n",
    "(ltn-lnn\n",
    "و\n",
    "ltc-lnc)\n",
    "شما باید لیستی مرتب از شناسه اسناد بر اساس امتیاز کسب شده برگردانید که امتیازات بر اساس توضیحات بالا باید محاسبه شوند.\n",
    "<br>\n",
    "<br>\n",
    "<font color=red size=6>\n",
    "جستجوی دقیق \n",
    "(phrasal search)\n",
    "</font>\n",
    "<br>\n",
    "این نوع جست‌وجو در قالب جست‌وجو‌های ترتیب‌دار قسمت قبل استفاده می‌شود. به این ترتیب که \n",
    "پرسمان ورودی ممکن است شامل تعدادی لغت و عبارات داخل گیومه باشد. اسناد بازیابی شده می‌بایست شامل عبارات داخل گیومه دقیقا به همان ترتیب و شکل آمده داخل گیومه باشند. \n",
    "<br>\n",
    "در صورت وجود چند عبارت داخل گیومه در پرسمان، ترتیب عبارات آمده داخل چند گیومه نسبت به هم لزومی ندارد حفظ شود. به \n",
    "عنوان نمونه برای پرسمان\n",
    "<br>\n",
    "\"q5 q4\" q3 \"q2 q1\"\n",
    "<br>\n",
    "سند\n",
    "<br>\n",
    "q3 q2 q1 q5 q4\n",
    "<br>\n",
    "مرتبط محسوب می‌شود. \n",
    "<br>\n",
    "جست‌وجو باید به این صورت باشد که ابتدا مجموعەی تمامی اسناد دارای عبارت‌های داخل گیومه پیدا می‌شود و سپس با استفاده از تمام لغات داخل پرسمان (شامل لغات داخل گیومه) بازیابی ترتیب دار با توضیحات آمده در قسمت قبل انجام شود.\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UnpNX56Jneu"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "(*امتیازی*)\n",
    "این بخش برای اصلاح پرسمان‌های ورودی است. تابع \n",
    "correct_query\n",
    "پرسمان کاربر  \n",
    "(query)\n",
    "را به عنوان ورودی می‌گیرد و در صورتی که کلماتی در پرسمان داخل واژه‌نامه‌ی نمایه وجود نداشته باشد آن کلمات را به شکل توضیح داده‌شده در بخش اصلاح پرسمان، با کلمات نزدیک به آن در واژه‌نامه جایگزین می‌کند و پرسمان اصلاح‌شده را برمی‌گرداند.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X5kybrGDJnev"
   },
   "outputs": [],
   "source": [
    "def correct_query(query):\n",
    "    correct_query = \"سلام حالا پرسمان درست شد.\"\n",
    "    return correct_query\n",
    "\n",
    "correct_query(\"شلام حالا برسهان درسک شد\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9HMqDMZJney"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش به جست و جوی پرسمان کلی اختصاص دارد. تابع \n",
    "search\n",
    "به عنوان اولین پارامتر پرسمان \n",
    "(query)\n",
    "را گرفته و جست و جو را روی آن انجام می‌دهد.\n",
    "در صورتی که درون پرسمان بخشی داخل\n",
    "\"\"\n",
    "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
    "پارامتر دوم ورودی روش محاسبه امتیاز \n",
    "(method)\n",
    "است که می‌تواند یکی از دو مقدار\n",
    "ltn-lnn\n",
    "و\n",
    "ltc-lnc\n",
    "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد.\n",
    "پارامتر سوم \n",
    "(weight)\n",
    "که یک عدد اعشاری است نسبت وزن امتیاز جست‌وجو در عنوان به امتیاز جست‌وجو در متن را مشخص می‌کند. که به طور پیش‌فرض این مقدار برابر ۲ است. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qsgz32PDJnez",
    "outputId": "b3b6ed43-794b-42e1-bc03-ee3e0134318b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4676' '3331']\n"
     ]
    }
   ],
   "source": [
    "def intersect_pos_list(p1, p2):\n",
    "    d1 = list(p1.keys())\n",
    "    d2 = list(p2.keys())\n",
    "    i = 0\n",
    "    j = 0\n",
    "    ################for title\n",
    "    merged_p_list = {}\n",
    "    while i < len(d1) and j < len(d2):\n",
    "        res = []\n",
    "        if d1[i] == d2[j]:\n",
    "            if \"title\" in p1[d1[i]].keys() and \"title\" in p2[d2[j]].keys():\n",
    "                pos1 = p1[d1[i]][\"title\"]\n",
    "                pos2 = p2[d2[j]][\"title\"]\n",
    "                ind1 = 0\n",
    "                ind2 = 0\n",
    "                while ind1 < len(pos1) and ind2 < len(pos2):\n",
    "                    if pos2[ind2] - pos1[ind1] == 1:\n",
    "                        if d1[i] not in merged_p_list.keys():\n",
    "                            merged_p_list[d1[i]] = {}\n",
    "                        res.append(pos1[ind1])\n",
    "                        ind1 += 1\n",
    "                        ind2 += 1\n",
    "                    elif pos2[ind2] - pos1[ind1] > 1:\n",
    "                        ind1 += 1\n",
    "                    else:\n",
    "                        ind2 += 1\n",
    "\n",
    "                if len(res) > 0:\n",
    "                    merged_p_list[d1[i]][\"title\"] = res\n",
    "            i += 1\n",
    "            j += 1\n",
    "\n",
    "        elif int(d1[i]) < int(d2[j]):\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "        ###########for text\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < len(d1) and j < len(d2):\n",
    "        res = []\n",
    "        if d1[i] == d2[j]:\n",
    "            if \"text\" in p1[d1[i]].keys() and \"text\" in p2[d2[j]].keys():\n",
    "                pos1 = p1[d1[i]][\"text\"]\n",
    "                pos2 = p2[d2[j]][\"text\"]\n",
    "                ind1 = 0\n",
    "                ind2 = 0\n",
    "                while ind1 < len(pos1) and ind2 < len(pos2):\n",
    "                    if pos2[ind2] - pos1[ind1] == 1:\n",
    "                        if d1[i] not in merged_p_list.keys():\n",
    "                            merged_p_list[d1[i]] = {}\n",
    "                        res.append(pos1[ind1])\n",
    "                        ind1 += 1\n",
    "                        ind2 += 1\n",
    "                    elif pos2[ind2] - pos1[ind1] > 1:\n",
    "                        ind1 += 1\n",
    "                    else:\n",
    "                        ind2 += 1\n",
    "\n",
    "                if len(res) > 0:\n",
    "                    merged_p_list[d1[i]][\"text\"] = res\n",
    "            i += 1\n",
    "            j += 1\n",
    "\n",
    "        elif int(d1[i]) < int(d2[j]):\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return merged_p_list\n",
    "\n",
    "def get_postings_for_query(tokenized_text):\n",
    "    postings = []\n",
    "    for i in range(len(tokenized_text)):\n",
    "        postings.append(get_posting_list(tokenized_text[i]))\n",
    "    return postings\n",
    "def get_q_query(q):\n",
    "    arr = np.array(list(q))\n",
    "    ind = np.where(arr == \"\\\"\")[0]\n",
    "    res = []\n",
    "    for i in range(0,len(ind) - 1,2):\n",
    "        current = arr[ind[i] + 1:ind[i + 1]]\n",
    "        st = ''.join(current)\n",
    "        res.append(st)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def find_exact(postings):\n",
    "    l = len(postings)\n",
    "    while l > 1:\n",
    "        new_postings = []\n",
    "        for i in range(len(postings) - 1):\n",
    "            new_postings.append(intersect_pos_list(postings[i], postings[i + 1]))\n",
    "        postings = new_postings\n",
    "        l = len(postings)\n",
    "    return postings[0]\n",
    "\n",
    "def find_exact_r(postings):\n",
    "    if len(postings == 1):\n",
    "        return postings\n",
    "    new_postings = []\n",
    "    for i in range(len(postings) - 1):\n",
    "        new_postings.append(intersect_pos_list(postings[i], postings[i + 1]))\n",
    "\n",
    "    return find_exact(new_postings)\n",
    "\n",
    "\n",
    "def intersect_without_position(p1, p2):\n",
    "    d1 = list(p1.keys())\n",
    "    d2 = list(p2.keys())\n",
    "    l1 = len(d1)\n",
    "    l2 = len(d2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    res = {}\n",
    "    while i < l1 and j < l2:\n",
    "        if d1[i] == d2[j]:\n",
    "            res[d1[i]] = p1[d1[i]]\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif d1[i] > d2[j]:\n",
    "            j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def get_docs_to_search(query):\n",
    "    in_quote = get_q_query(query)\n",
    "    if len(in_quote) > 0:\n",
    "        docs = []\n",
    "        for q in in_quote:\n",
    "            q = prepare_text(q)\n",
    "            postings = get_postings_for_query(q)\n",
    "            exact_search_res = find_exact(postings)\n",
    "            docs.append(exact_search_res)\n",
    "\n",
    "        indexes = docs[0]\n",
    "        for i in range(len(docs) - 1):\n",
    "            indexes = intersect_without_position(indexes, docs[i + 1])\n",
    "\n",
    "        return indexes\n",
    "    return None\n",
    "\n",
    "def search(query, method=\"ltn-lnn\", weight=2, number_of_return=20):\n",
    "    relevant_docs = []\n",
    "    docs = get_docs_to_search(query)\n",
    "    if docs is None:\n",
    "        scores = {}\n",
    "        lens_docs = {}\n",
    "        lens_query = 0\n",
    "        processed = prepare_text(query)\n",
    "        unqs, counts = np.unique(processed, return_counts=True)\n",
    "        weights = np.log10(counts) + 1\n",
    "        for i, word in enumerate(unqs):\n",
    "            q_weights = weights[i]\n",
    "            lens_query += (q_weights * q_weights)\n",
    "            posting = get_posting_list(word)\n",
    "            current_docs = list(posting.keys())\n",
    "            if len(current_docs) > 0:\n",
    "                for d in current_docs:\n",
    "                    tf_d = 0\n",
    "                    if \"title\" in posting[d].keys():\n",
    "                        tf_d += len(posting[d][\"title\"]) * weight\n",
    "                    if \"text\" in posting[d].keys():\n",
    "                        tf_d += len(posting[d][\"text\"])\n",
    "                    tf_d = np.log10(tf_d) + 1\n",
    "                    idf_d = np.log10(N / len(current_docs))\n",
    "                    if d in scores.keys():\n",
    "                        scores[d] += (tf_d * idf_d) * q_weights\n",
    "                        lens_docs[d] += (tf_d * idf_d) * (tf_d * idf_d)\n",
    "                    else:\n",
    "                        scores[d] = (tf_d * idf_d) * q_weights\n",
    "                        lens_docs[d] = (tf_d * idf_d) * (tf_d * idf_d)\n",
    "\n",
    "        relevant_docs = list(scores.keys())\n",
    "        if method == \"ltc-lnc\":\n",
    "            for i in range(len(scores)):\n",
    "                scores[relevant_docs[i]] = scores[relevant_docs[i]] / (np.sqrt(lens_docs[relevant_docs[i]] * np.sqrt(lens_query)))\n",
    "        scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1])}\n",
    "        relevant_docs = np.array(list(scores.keys()))\n",
    "        relevant_docs = np.flip(relevant_docs, axis = 0)\n",
    "        return relevant_docs[:min(len(relevant_docs), number_of_return)]\n",
    "    else:\n",
    "        scores = {}\n",
    "        lens_docs = {}\n",
    "        lens_query = 0\n",
    "        processed = prepare_text(query)\n",
    "        unqs, counts = np.unique(processed, return_counts=True)\n",
    "        weights = np.log10(counts) + 1\n",
    "        for i, word in enumerate(unqs):\n",
    "            q_weights = weights[i]\n",
    "            lens_query += (q_weights * q_weights)\n",
    "            posting = get_posting_list(word)\n",
    "            current_docs = list(posting.keys())\n",
    "            if len(current_docs) > 0:\n",
    "                for d in current_docs:\n",
    "                    if d in docs.keys():\n",
    "                        tf_d = 0\n",
    "                        if \"title\" in posting[d].keys():\n",
    "                            tf_d += len(posting[d][\"title\"]) * weight\n",
    "                        if \"text\" in posting[d].keys():\n",
    "                            tf_d += len(posting[d][\"text\"])\n",
    "                        tf_d = np.log10(tf_d) + 1\n",
    "                        idf_d = np.log10(N / len(current_docs))\n",
    "                        if d in scores.keys():\n",
    "                            scores[d] += (tf_d * idf_d) * q_weights\n",
    "                            lens_docs[d] += (tf_d * idf_d) * (tf_d * idf_d)\n",
    "                        else:\n",
    "                            scores[d] = (tf_d * idf_d) * q_weights\n",
    "                            lens_docs[d] = (tf_d * idf_d) * (tf_d * idf_d)\n",
    "\n",
    "        relevant_docs = list(scores.keys())\n",
    "        if method == \"ltc-lnc\":\n",
    "            for i in range(len(scores)):\n",
    "                scores[relevant_docs[i]] = scores[relevant_docs[i]] / (np.sqrt(lens_docs[relevant_docs[i]]) * np.sqrt(lens_query))\n",
    "        scores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1])}\n",
    "        relevant_docs = np.array(list(scores.keys()))\n",
    "\n",
    "        relevant_docs = np.flip(relevant_docs, axis = 0)\n",
    "        return relevant_docs[:min(len(relevant_docs), number_of_return)]\n",
    "\n",
    "result = search('\"مواد طبیعی\"', \"ltc-lnc\", 3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fTXhfuP0Jne5"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "این بخش به جست و جوی پرسمان بر اساس بخش اختصاص دارد. تابع \n",
    "detailed_search\n",
    "به عنوان دو پارامتر اول پرسمان بر روی عنوان \n",
    "(title_query)\n",
    "و پرسمان بر روی متن\n",
    "(text_query)\n",
    "را گرفته و جست و جو را روی آن‌ها انجام می‌دهد.\n",
    "در صورتی که درون پرسمان بخشی داخل\n",
    "\"\"\n",
    "قرار گیرد به این معنی است که آن بخش باید به صورت جست‌وجوی دقیق در جست‌وجو در نظر گرفته‌شود. \n",
    "پارامتر دوم ورودی روش محاسبه امتیاز \n",
    "(method)\n",
    "است که می‌تواند یکی از دو مقدار\n",
    "ltn-lnn\n",
    "و\n",
    "ltc-lnc\n",
    "را بپذیرد که به طور پیش‌فرض مقدار اول را می‌پذیرد. \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BmOjg1gYJne6",
    "outputId": "88f5ed0f-3ac6-4bba-c148-8e7aa4de09c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4568', '3997', '3996'], dtype='<U4')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\", number_of_return=20):\n",
    "    relevant_docs = []\n",
    "    docs = get_docs_to_search(text_query)\n",
    "    if docs is None:\n",
    "        scores_text = {}\n",
    "        lens_docs_text = {}\n",
    "        lens_query_text = 0\n",
    "        processed = prepare_text(text_query)\n",
    "        unqs, counts = np.unique(processed, return_counts=True)\n",
    "        weights = np.log10(counts) + 1\n",
    "        for i, word in enumerate(unqs):\n",
    "            q_weights = weights[i]\n",
    "            lens_query_text += (q_weights * q_weights)\n",
    "            posting = get_posting_list(word)\n",
    "            current_docs = []\n",
    "            for i in posting:\n",
    "                if \"text\" in posting[i].keys():\n",
    "                    current_docs.append(i)\n",
    "            if len(current_docs) > 0:\n",
    "                for d in current_docs:\n",
    "                    tf_d = 0\n",
    "                    tf_d += len(posting[d][\"text\"])\n",
    "                    tf_d = np.log10(tf_d) + 1\n",
    "                    idf_d = np.log10(N / len(current_docs))\n",
    "                    if d in scores_text.keys():\n",
    "                        scores_text[d] += (tf_d * idf_d) * q_weights\n",
    "                        lens_docs_text[d] += (tf_d * idf_d) * (tf_d * idf_d)\n",
    "                    else:\n",
    "                        scores_text[d] = (tf_d * idf_d) * q_weights\n",
    "                        lens_docs_text[d] = (tf_d * idf_d) * (tf_d * idf_d)\n",
    "\n",
    "        relevant_docs_text = list(scores_text.keys())\n",
    "        if method == \"ltc-lnc\":\n",
    "            for i in range(len(scores_text)):\n",
    "                scores_text[relevant_docs_text[i]] = scores_text[relevant_docs_text[i]] / (\n",
    "                    np.sqrt(lens_docs_text[relevant_docs_text[i]] * np.sqrt(lens_query_text)))\n",
    "\n",
    "\n",
    "    else:\n",
    "        scores_text = {}\n",
    "        lens_docs_text = {}\n",
    "        lens_query_text = 0\n",
    "        processed = prepare_text(title_query)\n",
    "        unqs, counts = np.unique(processed, return_counts=True)\n",
    "        weights = np.log10(counts) + 1\n",
    "        for i, word in enumerate(unqs):\n",
    "            q_weights = weights[i]\n",
    "            lens_query_text += (q_weights * q_weights)\n",
    "            posting = get_posting_list(word)\n",
    "            current_docs = []\n",
    "            for i in posting:\n",
    "                if \"text\" in i.keys():\n",
    "                    current_docs.append(i)\n",
    "            if len(current_docs) > 0:\n",
    "                for d in current_docs:\n",
    "                    if d in docs.keys():\n",
    "                        tf_d = 0\n",
    "                        if \"text\" in posting[d].keys():\n",
    "                            tf_d += len(posting[d][\"text\"])\n",
    "                        tf_d = np.log10(tf_d) + 1\n",
    "                        idf_d = np.log10(N / len(current_docs))\n",
    "                        if d in scores_text.keys():\n",
    "                            scores_text[d] += (tf_d * idf_d) * q_weights\n",
    "                            lens_docs_text[d] += (tf_d * idf_d) * (tf_d * idf_d)\n",
    "                        else:\n",
    "                            scores_text[d] = (tf_d * idf_d) * q_weights\n",
    "                            lens_docs_text[d] = (tf_d * idf_d) * (tf_d * idf_d)\n",
    "\n",
    "        relevant_docs_text = list(scores_text.keys())\n",
    "        if method == \"ltc-lnc\":\n",
    "            for i in range(len(scores_text)):\n",
    "                scores_text[relevant_docs_text[i]] = scores_text[relevant_docs_text[i]] / (\n",
    "                    np.sqrt(lens_docs_text[relevant_docs_text[i]] * np.sqrt(lens_query_text)))\n",
    "\n",
    "\n",
    "    relevant_docs = []\n",
    "    docs = get_docs_to_search(title_query)\n",
    "    if docs is None:\n",
    "        scores_title = {}\n",
    "        lens_docs_title = {}\n",
    "        lens_query_title = 0\n",
    "        processed = prepare_text(title_query)\n",
    "        unqs, counts = np.unique(processed, return_counts=True)\n",
    "        weights = np.log10(counts) + 1\n",
    "        for i, word in enumerate(unqs):\n",
    "            q_weights = weights[i]\n",
    "            lens_query_title += (q_weights * q_weights)\n",
    "            posting = get_posting_list(word)\n",
    "            current_docs = []\n",
    "            for i in posting:\n",
    "                if \"title\" in posting[i].keys():\n",
    "                    current_docs.append(i)\n",
    "            if len(current_docs) > 0:\n",
    "                for d in current_docs:\n",
    "                    tf_d = 0\n",
    "\n",
    "                    tf_d += len(posting[d][\"title\"])\n",
    "                    tf_d = np.log10(tf_d) + 1\n",
    "                    idf_d = np.log10(N / len(current_docs))\n",
    "                    if d in scores_title.keys():\n",
    "                        scores_title[d] += (tf_d * idf_d) * q_weights\n",
    "                        lens_docs_title[d] += (tf_d * idf_d) * (tf_d * idf_d)\n",
    "                    else:\n",
    "                        scores_title[d] = (tf_d * idf_d) * q_weights\n",
    "                        lens_docs_title[d] = (tf_d * idf_d) * (tf_d * idf_d)\n",
    "\n",
    "        relevant_docs_title = list(scores_title.keys())\n",
    "        if method == \"ltc-lnc\":\n",
    "            for i in range(len(scores_title)):\n",
    "                scores_title[relevant_docs_title[i]] = scores_title[relevant_docs_title[i]] / (\n",
    "                    np.sqrt(lens_docs_title[relevant_docs_title[i]] * np.sqrt(lens_query_title)))\n",
    "\n",
    "    else:\n",
    "        scores_title = {}\n",
    "        lens_docs_title = {}\n",
    "        lens_query_title = 0\n",
    "        processed = prepare_text(title_query)\n",
    "        unqs, counts = np.unique(processed, return_counts=True)\n",
    "        weights = np.log10(counts) + 1\n",
    "        for i, word in enumerate(unqs):\n",
    "            q_weights = weights[i]\n",
    "            lens_query_title += (q_weights * q_weights)\n",
    "            posting = get_posting_list(word)\n",
    "            current_docs = []\n",
    "            for i in posting:\n",
    "                if \"title\" in posting[i].keys():\n",
    "                    current_docs.append(i)\n",
    "            if len(current_docs) > 0:\n",
    "                for d in current_docs:\n",
    "                    if d in docs.keys():\n",
    "                        tf_d = 0\n",
    "                        if \"title\" in posting[d].keys():\n",
    "                            tf_d += len(posting[d][\"title\"])\n",
    "                        tf_d = np.log10(tf_d) + 1\n",
    "                        idf_d = np.log10(N / len(current_docs))\n",
    "                        if d in scores_title.keys():\n",
    "                            scores_title[d] += (tf_d * idf_d) * q_weights\n",
    "                            lens_docs_title[d] += (tf_d * idf_d) * (tf_d * idf_d)\n",
    "                        else:\n",
    "                            scores_title[d] = (tf_d * idf_d) * q_weights\n",
    "                            lens_docs_title[d] = (tf_d * idf_d) * (tf_d * idf_d)\n",
    "\n",
    "        relevant_docs_title = list(scores_title.keys())\n",
    "        if method == \"ltc-lnc\":\n",
    "            for i in range(len(scores_title)):\n",
    "                scores_title[relevant_docs_title[i]] = scores_title[relevant_docs_title[i]] / (\n",
    "                    np.sqrt(lens_docs_title[relevant_docs_title[i]] * np.sqrt(lens_query_title)))\n",
    "\n",
    "    title_and_text = intersect_without_position(scores_title, scores_text)\n",
    "    title_and_text = {k: v for k, v in sorted(title_and_text.items(), key=lambda item: item[1])}\n",
    "    relevant_docs = np.array(list(title_and_text.keys()))\n",
    "    relevant_docs = np.flip(relevant_docs, axis = 0)\n",
    "\n",
    "\n",
    "    return relevant_docs[:min(len(relevant_docs), number_of_return)]\n",
    "\n",
    "detailed_search('جغرافیا', 'علوم اجتماعی', \"ltc-lnc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jpAO-xvEJne-"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center> (20 نمره) بخش چهارم: ارزیابی سیستم</div>\n",
    "</font>\n",
    "<hr>\n",
    "سیستم شما باید قادر باشد با استفاده از معیارهای\n",
    "<ol>\n",
    "<li>\n",
    "MAP\n",
    "</li>\n",
    "<li>\n",
    "F-Measure\n",
    "</li>\n",
    "<li>\n",
    "R-Precision\n",
    "</li>\n",
    "<li>\n",
    "NDCG\n",
    "</li>\n",
    "</ol>\n",
    "نتایج را ارزیابی کند. برای ارزیابی تعدادی پرسمان و نتایج آنها در اختیار شما قرار گرفته است که باید پاسخ سیستم‌تان به پرسمان ها را با نتایج متناظر هر پرسمان ارزیابی و مقایسه کنید. در صورتی که کل پرسمان در یک خط آمده بود به این معنی است که پرسمان کلی است و تابع\n",
    "search \n",
    "باید برای آن فراخوانی شود و در صورتی که پرسمان در  دو خط آمده بود، خط اول پرسمان عنوان و خط دوم پرسمان متن خواهد بود و باید تابع\n",
    "detailed_search\n",
    "را برای آن فراخوانی کنید و نتیجه را ارزیابی کنید.\n",
    "<br>\n",
    "توجه کنید که این چهار معیار را جداگانه و مستقل از بقیه نیز بتوانید حساب کنید. حداکثر سند بازیابی شده را ۱۵ قرار دهید.    \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l52cf_R-Jne_"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در قطعه کد بخش زیر به ازای هر معیار یک تابع آمده است که به عنوان ورودی شماره پرسمان را می‌گیرد و با خواندن پرسمان و لیست مرتب سند‌های مرتبط با آن از فایل‌های مربوطه، جستجوی پرسمان را با توجه به نوع پرسمان انجام می‌دهد، نتیجه را ارزیابی کرده و مقدار محاسبه شده معیار را بر می‌گرداند.\n",
    "در صورتی که در ورودی به جای شماره پرسمان رشته‌ی\n",
    "all\n",
    "آمده بود ارزیابی باید بر روی تمامی اسناد صورت گیرد و میانگین مقادیر معیار ازیابی برای همه پرسمان‌ها به عنوان خروجی برگردانده شود.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqaOk4ESJnfA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n",
      "0.6659420162461929\n",
      "0.6130248643281756\n",
      "0.8018082611832612\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def R_Precision(query_id='all'):\n",
    "    \n",
    "    result = 0\n",
    "    num_of_queries = 0\n",
    "    r_ps = 0\n",
    "    if query_id == 'all':\n",
    "\n",
    "        for file in glob.glob('data\\\\queries\\\\*.txt'):\n",
    "            num_of_queries += 1\n",
    "            with open(file, 'r', encoding=\"utf-8\") as query_file:\n",
    "                query = query_file.read()\n",
    "                query = query.split(\"\\n\")\n",
    "                rl_file = file.replace(\"queries\", \"relevance\")\n",
    "            with open(rl_file, 'r') as rf:\n",
    "                rl = rf.read().split(\", \")\n",
    "            if len(query) == 1:\n",
    "                relevant_docs = search(query[0], method=\"ltn-lnn\")\n",
    "            else:\n",
    "                relevant_docs = detailed_search(query[0], query[1])\n",
    "\n",
    "            R = len(rl)\n",
    "            i = 0\n",
    "            r = 0\n",
    "            while i < R and i < len(relevant_docs):\n",
    "                if relevant_docs[i] in rl:\n",
    "                    r += 1\n",
    "                i += 1\n",
    "            r_ps += (r / R)\n",
    "        return r_ps / num_of_queries\n",
    "    else:\n",
    "        file = 'data\\\\queries\\\\%s.txt' % (query_id,)\n",
    "        with open(file, 'r', encoding=\"utf-8\") as query_file:\n",
    "            query = query_file.read()\n",
    "            query = query.split(\"\\n\")\n",
    "            rl_file = file.replace(\"queries\", \"relevance\")\n",
    "        with open(rl_file, 'r') as rf:\n",
    "            rl = rf.read().split(\", \")\n",
    "        if len(query) == 1:\n",
    "            relevant_docs = search(query[0], method=\"ltn-lnn\")\n",
    "        else:\n",
    "            relevant_docs = detailed_search(query[0], query[1])\n",
    "        R = len(rl)\n",
    "        i = 0\n",
    "        r = 0\n",
    "        while i < R and i < len(relevant_docs):\n",
    "            if relevant_docs[i] in rl:\n",
    "                r += 1\n",
    "            i += 1\n",
    "        return r / R\n",
    "    \n",
    "def F_measure(query_id='all'):\n",
    "    result = 0\n",
    "    num_of_queries = 0\n",
    "    if query_id == 'all':\n",
    "        for file in glob.glob('data\\\\queries\\\\*.txt'):\n",
    "            num_of_queries += 1\n",
    "            with open(file, 'r', encoding=\"utf-8\") as query_file:\n",
    "               query = query_file.read()\n",
    "               query = query.split(\"\\n\")\n",
    "               rl_file = file.replace(\"queries\", \"relevance\")\n",
    "            with open(rl_file, 'r') as rf:\n",
    "                rl = rf.read().split(\", \")\n",
    "            if len(query) == 1:\n",
    "                relevant_docs = search(query[0], method=\"ltc-lnc\")\n",
    "            else:\n",
    "                relevant_docs = detailed_search(query[0], query[1])\n",
    "\n",
    "            tp, tn, fp, fn = compute_confusion_matrix(relevant_docs, rl)\n",
    "            recall = 0\n",
    "            if tp + fn > 0:\n",
    "                recall = tp / (tp + fn)\n",
    "            percision = 0\n",
    "            if tp + fp > 0:\n",
    "                percision = tp / (tp + fp)\n",
    "            f_mean = 0\n",
    "            if percision + recall > 0:\n",
    "                f_mean = (2 * percision * recall) / (percision + recall)\n",
    "            result += f_mean\n",
    "\n",
    "        return result / num_of_queries\n",
    "    else:\n",
    "        file = 'data\\\\queries\\\\%s.txt'%(query_id,)\n",
    "        with open(file, 'r', encoding=\"utf-8\") as query_file:\n",
    "            query = query_file.read()\n",
    "            query = query.split(\"\\n\")\n",
    "            rl_file = file.replace(\"queries\", \"relevance\")\n",
    "        with open(rl_file, 'r') as rf:\n",
    "            rl = rf.read().split(\", \")\n",
    "        if len(query) == 1:\n",
    "            relevant_docs = search(query[0], method=\"ltc-lnc\")\n",
    "        else:\n",
    "            relevant_docs = detailed_search(query[0], query[1])\n",
    "        tp, tn, fp, fn = compute_confusion_matrix(relevant_docs, rl)\n",
    "        recall = 0\n",
    "        if tp + fn > 0:\n",
    "            recall = tp / (tp + fn)\n",
    "        percision = 0\n",
    "        if tp + fp > 0:\n",
    "            percision = tp / (tp + fp)\n",
    "        f_mean = 0\n",
    "        if percision + recall > 0:\n",
    "            f_mean = (2 * percision * recall) / (percision + recall)\n",
    "        return f_mean\n",
    "\n",
    "def compute_confusion_matrix(answer, relevant):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for d in answer:\n",
    "        if d in relevant:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    fn = len(relevant) - tp\n",
    "    tn = (N - len(answer)) - fn\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def MAP(query_id='all'):\n",
    "    result = 0\n",
    "    num_of_queries = 0\n",
    "    precisions = []\n",
    "    if query_id == 'all':\n",
    "\n",
    "        for file in glob.glob('data\\\\queries\\\\*.txt'):\n",
    "            num_of_queries += 1\n",
    "            with open(file, 'r', encoding=\"utf-8\") as query_file:\n",
    "                query = query_file.read()\n",
    "                query = query.split(\"\\n\")\n",
    "                rl_file = file.replace(\"queries\", \"relevance\")\n",
    "            with open(rl_file, 'r') as rf:\n",
    "                rl = rf.read().split(\", \")\n",
    "            if len(query) == 1:\n",
    "                relevant_docs = search(query[0], method=\"ltn-lnn\")\n",
    "            else:\n",
    "                relevant_docs = detailed_search(query[0], query[1])\n",
    "\n",
    "            precision_score = 0\n",
    "            number_of_relevant = 0\n",
    "\n",
    "\n",
    "            for i,d in enumerate(relevant_docs):\n",
    "                if d in rl:\n",
    "                    number_of_relevant += 1\n",
    "                    precision_score += number_of_relevant / (i + 1)\n",
    "            precisions.append(precision_score / len(rl))\n",
    "\n",
    "        return np.mean(precisions)\n",
    "    else:\n",
    "        file = 'data\\\\queries\\\\%s.txt' % (query_id,)\n",
    "        with open(file, 'r', encoding=\"utf-8\") as query_file:\n",
    "            query = query_file.read()\n",
    "            query = query.split(\"\\n\")\n",
    "            rl_file = file.replace(\"queries\", \"relevance\")\n",
    "        with open(rl_file, 'r') as rf:\n",
    "            rl = rf.read().split(\", \")\n",
    "        if len(query) == 1:\n",
    "            relevant_docs = search(query[0], method=\"ltn-lnn\")\n",
    "        else:\n",
    "            relevant_docs = detailed_search(query[0], query[1])\n",
    "        precision_score = 0\n",
    "        number_of_relevant = 0\n",
    "\n",
    "        for i, d in enumerate(relevant_docs):\n",
    "            if d in rl:\n",
    "                number_of_relevant += 1\n",
    "                precision_score += number_of_relevant / (i + 1)\n",
    "        return precision_score / len(rl)\n",
    "\n",
    "def NDCG(query_id='all'):\n",
    "    result = 0\n",
    "    num_of_queries = 0\n",
    "    DCGS = []\n",
    "\n",
    "    for file in glob.glob('data\\\\queries\\\\*.txt'):\n",
    "        num_of_queries += 1\n",
    "        with open(file, 'r', encoding=\"utf-8\") as query_file:\n",
    "            query = query_file.read()\n",
    "            query = query.split(\"\\n\")\n",
    "            rl_file = file.replace(\"queries\", \"relevance\")\n",
    "        with open(rl_file, 'r') as rf:\n",
    "            rl = rf.read().split(\", \")\n",
    "        if len(query) == 1:\n",
    "            relevant_docs = search(query[0], method=\"ltn-lnn\")\n",
    "        else:\n",
    "            relevant_docs = detailed_search(query[0], query[1])\n",
    "\n",
    "\n",
    "        dcg = 0\n",
    "        for i,d in enumerate(relevant_docs):\n",
    "            if d in rl:\n",
    "                dcg += 1 / np.log2(2 + i)\n",
    "        DCGS.append(dcg)\n",
    "    DCGS = np.divide(DCGS, max(DCGS))\n",
    "    if query_id == \"all\":\n",
    "        return np.mean(DCGS)\n",
    "    return DCGS[query_id - 1]\n",
    "\n",
    "print(R_Precision(1))\n",
    "print(NDCG())\n",
    "print(F_measure())\n",
    "print(MAP(1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUs450l5JnfD"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>نکات پایانی</div>\n",
    "</font>\n",
    "<hr>\n",
    "۱- سیستم را به صورت بهینه پیاده سازی کنید تا در زمان کمتری بارگذاری و نمایه سازی و … را انجام دهد.\n",
    "<br>\n",
    "۲- فایل‌های \n",
    "ipynb\n",
    "و پایتون \n",
    "پاسخ تمرین را (بدون داده‌ها) به صورت فایل فشرده در کوئرا بارگذاری کنید.\n",
    "<br>\n",
    "۳- اشکالات خود از فاز اول پروژه را در زیر پست مربوط به این تمرین بپرسید.\n",
    "<br>\n",
    "۴- نام فایل ارسالی به صورت Project1-StudentNumber باشد.\n",
    "<br>\n",
    "۵- موعد تحویل تمرین تا ساعت ۲۳:۵۹ پانزدهم فروردین می‌باشد و جریمەی تأخیر مطابق با قوانینی که در سایت درس قرار داده شدەاست ، خواهد بود.\n",
    "<br>\n",
    "۶- در صورت مشاهده تقلب، طبق قوانین دانشکده با شما برخورد خواهد شد.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2AOY8hCJnfE"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\"><font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B titr\" size=30>\n",
    "<p></p>\n",
    "<font color=#FF7500> \n",
    "موفق باشید\n",
    ":)\n",
    "<br>\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "MIRProjectPhase1Spring99_ژخحغ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
